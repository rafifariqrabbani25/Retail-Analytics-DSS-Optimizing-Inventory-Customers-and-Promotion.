{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13562037,"sourceType":"datasetVersion","datasetId":8614538}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rafifariqrabbani/retail-decision-support-system?scriptVersionId=296683772\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"In large scale retail operations, the increasing variety of products and the complexity of customer demand make inventory and operational control a critical managerial challenge. Simply identifying high selling or high value products is no longer sufficient to support effective decision making. Instead, systematic classification methods are required to prioritize managerial attention, allocate resources efficiently, and mitigate operational risks.\n\nThis study utilizes transaction-level data from a global supermarket dataset to perform a comprehensive operational analysis, covering customer behavior, geographic performance, logistics efficiency, and product demand patterns. In particular, classification techniques such as ABC and FSN analysis are employed not merely to recognize product importance, but to differentiate control policies based on economic impact and demand movement characteristics. By integrating these classifications with time series, geographic, and customer segmentation analyses, this study aims to provide actionable insights for inventory planning, distribution strategy, and overall supply chain optimization.\n\nDue to data limitations, many analytical decisions and interpretations are based on qualitative judgment rather than strictly quantitative rules.\n\nThis study covers several key analytical dimensions derived from the supermarket transaction dataset:\n1. ABC Classification to prioritize products based on their cumulative contribution to total sales value.\n2. FSN Classification to distinguish products based on demand movement speed.\n3. Syntetos-Boylan framework (using ADI-CV) to identify demand behavior in frequency and quantity\n4. Time Series and Seasonality Analysis to identify temporal demand patterns, and seasonal effects.\n5. Product Performance Analysis to compare product categories, sub-categories, and individual products based on sales contribution.\n6. Customer Segmentation, RFM & CRV Analysis to identify differences in purchasing behavior, customer value, and retention patterns.\n7. Geographic Performance Analysis to evaluate regional, state, city, and postal code level sales concentration and market potential.\n8. Repeat Purchase Analysis to evaluate customer loyalty, repurchase behavior, and customer lifetime characteristics.\n\n\nReferences:\n- Khajvand, M., Zolfaghar, K., Ashoori, S., & Alizadeh, S. (2011). Estimating customer lifetime value based on RFM analysis of customer purchase behavior: Case study. Procedia computer science, 3, 57-63.\n- Kljajić, M., D. Kofjač, A. Škraba and V. Rejec. (2004). “Warehouse optimization in an uncertain environment,” Proceedings of the 22nd International Conference of the System Dynamics Society, Oxford, England.\n- Mohajan, H. (2017). An analysis on BCG growth sharing matrix.\n- Parekh, S., Lee, J., and Kozman, T. (2008). A decision support system for inventory management. Southwest Decision Sciences Institute, 513-522\n- Syntetos, A. A., Boylan, J. E., & Croston, J. D. (2005). On the categorization of demand patterns. Journal of the operational research society, 56(5), 495-503.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:03:11.572379Z","iopub.execute_input":"2026-02-09T08:03:11.572805Z","iopub.status.idle":"2026-02-09T08:03:14.01528Z","shell.execute_reply.started":"2026-02-09T08:03:11.572758Z","shell.execute_reply":"2026-02-09T08:03:14.013913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1) INVENTORY CLASSIFICATION ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"Section 1 focuses on developing an integrated inventory classification framework by combining ABC, FSN, and ADI–CV analyses to identify product criticality, movement speed, and demand patterns.\nThis section aims to support differentiated inventory control, service level setting, and replenishment strategies based on the economic importance and demand behavior of each product.","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# SECTION 1: INVENTORY CLASSIFICATION ANALYSIS\n# Complete ABC, FSN, ADI, and CV Classification System\n# =================================================================================\n\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the data\nprint(\"=\"*80)\nprint(\"SECTION 1: INVENTORY CLASSIFICATION ANALYSIS\")\nprint(\"ABC | FSN | ADI | CV Classification\")\nprint(\"=\"*80)\n\n# Load from CSV file\ndata = \"/kaggle/input/super-market-analyses/supermarket.csv\"\ndf = pd.read_csv(data)\n\n# Convert Order Date to datetime\ndf['Order Date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y')\n\nprint(f\"\\n✓ Data loaded successfully\")\nprint(f\"  Total records: {len(df)}\")\nprint(f\"  Unique products: {df['Product ID'].nunique()}\")\nprint(f\"  Date range: {df['Order Date'].min().strftime('%Y-%m-%d')} to {df['Order Date'].max().strftime('%Y-%m-%d')}\")\n\n# =================================================================================\n# STEP 1: AGGREGATE DATA BY PRODUCT\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 1: Product Aggregation\")\nprint(f\"{'-'*80}\")\n\nproduct_agg = df.groupby('Product ID').agg({\n    'Product Name': 'first',\n    'Category': 'first',\n    'Sub-Category': 'first',\n    'Sales': ['sum', 'count', 'mean', 'std'],\n    'Order Date': lambda x: (x.max() - x.min()).days\n}).reset_index()\n\nproduct_agg.columns = ['Product_ID', 'Product_Name', 'Category', 'Sub_Category', \n                        'Total_Sales', 'Order_Count', 'Avg_Sales', 'Std_Sales', 'Date_Range_Days']\n\n# Calculate additional metrics\nproduct_agg['Quantity'] = product_agg['Order_Count']  # Using order count as proxy for quantity\nproduct_agg['Unit_Cost'] = product_agg['Total_Sales'] / product_agg['Quantity']\n\nprint(f\"✓ Aggregated {len(product_agg)} unique products\")\n\n# =================================================================================\n# STEP 2: ABC CLASSIFICATION (Criticality Analysis)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2: ABC Classification (Criticality - Dollar Usage)\")\nprint(f\"{'-'*80}\")\n\n# Calculate dollar usage\nproduct_agg['Dollar_Usage'] = product_agg['Total_Sales']\nproduct_agg = product_agg.sort_values('Dollar_Usage', ascending=False).reset_index(drop=True)\n\n# Calculate percentages\ntotal_dollar_usage = product_agg['Dollar_Usage'].sum()\ntotal_quantity = product_agg['Quantity'].sum()\n\nproduct_agg['Pct_Cost'] = (product_agg['Dollar_Usage'] / total_dollar_usage) * 100\nproduct_agg['Pct_Quantity'] = (product_agg['Quantity'] / total_quantity) * 100\nproduct_agg['Cum_Pct_Cost'] = product_agg['Pct_Cost'].cumsum()\nproduct_agg['Cum_Pct_Quantity'] = product_agg['Pct_Quantity'].cumsum()\n\n# ABC Classification logic\ndef classify_abc(cum_pct):\n    if cum_pct <= 70:\n        return 'A'\n    elif cum_pct <= 90:\n        return 'B'\n    else:\n        return 'C'\n\nproduct_agg['ABC_Class'] = product_agg['Cum_Pct_Cost'].apply(classify_abc)\n\n# Summary\nabc_summary = product_agg.groupby('ABC_Class').agg({\n    'Product_ID': 'count',\n    'Pct_Cost': 'sum',\n    'Pct_Quantity': 'sum'\n}).round(2)\n\nprint(f\"\\n ABC Classification Summary:\")\nprint(f\" {abc_summary}\")\nprint(f\"\\n ✓ Class A: {len(product_agg[product_agg['ABC_Class']=='A'])} products (High Value)\")\nprint(f\" ✓ Class B: {len(product_agg[product_agg['ABC_Class']=='B'])} products (Medium Value)\")\nprint(f\" ✓ Class C: {len(product_agg[product_agg['ABC_Class']=='C'])} products (Low Value)\")\n\n# =================================================================================\n# STEP 3: FSN CLASSIFICATION (Velocity Analysis)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 3: FSN Classification (Fast/Slow/Non-Moving)\")\nprint(f\"{'-'*80}\")\n\n# Calculate total demand (using quantity as demand)\nproduct_agg['Total_Demand'] = product_agg['Quantity']\n\n# Sort by demand for FSN\nfsn_sorted = product_agg.sort_values('Total_Demand', ascending=True).reset_index(drop=True)\n\n# Calculate quartiles\nn = len(fsn_sorted)\nQ1 = fsn_sorted['Total_Demand'].quantile(0.25)\nQ3 = fsn_sorted['Total_Demand'].quantile(0.75)\n\nprint(f\"\\n FSN Quartiles:\")\nprint(f\"  Q1 (25th percentile): {Q1:.2f}\")\nprint(f\"  Q3 (75th percentile): {Q3:.2f}\")\n\n# FSN Classification logic\ndef classify_fsn(demand, q1, q3):\n    if demand > q3:\n        return 'F'  # Fast Moving\n    elif demand < q1:\n        return 'N'  # Non-Moving\n    else:\n        return 'S'  # Slow Moving\n\nproduct_agg['FSN_Class'] = product_agg['Total_Demand'].apply(\n    lambda x: classify_fsn(x, Q1, Q3)\n)\n\n# Summary\nfsn_summary = product_agg.groupby('FSN_Class').agg({\n    'Product_ID': 'count',\n    'Total_Demand': ['min', 'max', 'mean']\n}).round(2)\n\nprint(f\"\\n FSN Classification Summary:\")\nprint(f\" {fsn_summary}\")\nprint(f\"\\n ✓ Class F (Fast): {len(product_agg[product_agg['FSN_Class']=='F'])} products\")\nprint(f\" ✓ Class S (Slow): {len(product_agg[product_agg['FSN_Class']=='S'])} products\")\nprint(f\" ✓ Class N (Non-Moving): {len(product_agg[product_agg['FSN_Class']=='N'])} products\")\n\n# =================================================================================\n# STEP 4: ADI CALCULATION (Average Demand Interval)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 4: ADI Calculation (Average Demand Interval)\")\nprint(f\"{'-'*80}\")\n\n# ADI = Number of periods / Total periods with non-zero demand\n# Calculate periods with demand (transactions) vs total available periods\n# Assuming daily periods in the date range\n\ndef calculate_adi(row):\n    \"\"\"\n    ADI = Total periods / Number of periods with demand\n    Using days as periods\n    \"\"\"\n    total_periods = row['Date_Range_Days'] + 1  # +1 to include both start and end date\n    periods_with_demand = row['Order_Count']\n    \n    if periods_with_demand == 0 or total_periods == 0:\n        return 0\n    \n    adi = total_periods / periods_with_demand\n    return adi\n\nproduct_agg['ADI'] = product_agg.apply(calculate_adi, axis=1)\n\n# Handle edge cases\nproduct_agg['ADI'] = product_agg['ADI'].replace([np.inf, -np.inf], 0)\nproduct_agg['ADI'] = product_agg['ADI'].fillna(0)\n\nprint(f\"\\n ADI Statistics:\")\nprint(f\"  Mean ADI: {product_agg['ADI'].mean():.2f}\")\nprint(f\"  Median ADI: {product_agg['ADI'].median():.2f}\")\nprint(f\"  Min ADI: {product_agg['ADI'].min():.2f}\")\nprint(f\"  Max ADI: {product_agg['ADI'].max():.2f}\")\nprint(f\"\\n  Interpretation:\")\nprint(f\"    ADI > 1.32: Intermittent/Lumpy demand\")\nprint(f\"    ADI <= 1.32: Smooth/Erratic demand\")\n\n# =================================================================================\n# STEP 5: CV CALCULATION (Coefficient of Variation)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 5: CV Calculation (Coefficient of Variation)\")\nprint(f\"{'-'*80}\")\n\n# CV = Standard Deviation of Demand / Mean Demand\n# Using Sales as proxy for demand\nproduct_agg['CV_Squared'] = (product_agg['Std_Sales'] / product_agg['Avg_Sales']) ** 2\n\n# Handle cases where Avg_Sales is 0 or Std is NaN\nproduct_agg['CV_Squared'] = product_agg['CV_Squared'].replace([np.inf, -np.inf], 0)\nproduct_agg['CV_Squared'] = product_agg['CV_Squared'].fillna(0)\n\nprint(f\"\\n CV²Statistics:\")\nprint(f\"  Mean CV²: {product_agg['CV_Squared'].mean():.2f}\")\nprint(f\"  Median CV²: {product_agg['CV_Squared'].median():.2f}\")\nprint(f\"  Min CV²: {product_agg['CV_Squared'].min():.2f}\")\nprint(f\"  Max CV²: {product_agg['CV_Squared'].max():.2f}\")\nprint(f\"\\n  Interpretation:\")\nprint(f\"    CV² > 0.49: High variability (Erratic/Lumpy)\")\nprint(f\"    CV² <= 0.49: Low variability (Smooth/Intermittent)\")\n\n# =================================================================================\n# STEP 6: DEMAND PATTERN CLASSIFICATION (ADI-CV Matrix)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 6: Demand Pattern Classification (ADI-CV Matrix)\")\nprint(f\"{'-'*80}\")\n\n# Classification based on ADI and CV² thresholds:\n# ADI > 1.32 & CV² > 0.49  → Lumpy\n# ADI > 1.32 & CV² <= 0.49 → Intermittent\n# ADI <= 1.32 & CV² > 0.49 → Erratic\n# ADI <= 1.32 & CV² <= 0.49 → Smooth\n\ndef classify_demand_pattern(row):\n    adi = row['ADI']\n    cv_sq = row['CV_Squared']\n    \n    if adi > 1.32 and cv_sq > 0.49:\n        return 'Lumpy'\n    elif adi > 1.32 and cv_sq <= 0.49:\n        return 'Intermittent'\n    elif adi <= 1.32 and cv_sq > 0.49:\n        return 'Erratic'\n    else:  # adi <= 1.32 and cv_sq <= 0.49\n        return 'Smooth'\n\nproduct_agg['Demand_Pattern'] = product_agg.apply(classify_demand_pattern, axis=1)\n\n# Summary\npattern_summary = product_agg['Demand_Pattern'].value_counts()\nprint(f\"\\n Demand Pattern Classification Summary:\")\nfor pattern, count in pattern_summary.items():\n    pct = (count / len(product_agg)) * 100\n    print(f\"  {pattern}: {count} products ({pct:.1f}%)\")\n\nprint(f\"\\n Pattern Descriptions:\")\nprint(f\"  • Smooth: Regular, predictable demand (ADI≤1.32, CV²≤0.49)\")\nprint(f\"  • Erratic: Frequent but variable demand (ADI≤1.32, CV²>0.49)\")\nprint(f\"  • Intermittent: Infrequent, stable demand (ADI>1.32, CV²≤0.49)\")\nprint(f\"  • Lumpy: Infrequent, highly variable demand (ADI>1.32, CV²>0.49)\")\n\n# =================================================================================\n# STEP 7: COMBINED CLASSIFICATION WITH RECOMMENDATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 7: Combined Classification Matrix with Recommendations\")\nprint(f\"{'-'*80}\")\n\n# Create combined classification: ABC-FSN-DemandPattern\nproduct_agg['Combined_Class'] = (\n    product_agg['ABC_Class'] + '-' + \n    product_agg['FSN_Class'] + '-' + \n    product_agg['Demand_Pattern']\n)\n\n# =================================================================================\n# SERVICE LEVEL RECOMMENDATIONS BASED ON ABC CLASSIFICATION\n# =================================================================================\ndef assign_service_level(abc_class):\n    \"\"\"\n    Assign target service level based on ABC classification\n    Higher value items (A) require higher service levels\n    \"\"\"\n    service_levels = {\n        'A': 99.0,  # 99% service level for critical items\n        'B': 95.0,  # 95% service level for moderate items\n        'C': 90.0   # 90% service level for low-value items\n    }\n    return service_levels[abc_class]\n\nproduct_agg['Service_Level'] = product_agg['ABC_Class'].apply(assign_service_level)\n\n# Z-score lookup table for service levels (for normal distribution)\nz_scores = {\n    99.0: 2.33,  # 99% service level\n    95.0: 1.65,  # 95% service level\n    90.0: 1.28   # 90% service level\n}\n\nproduct_agg['Z_Score'] = product_agg['Service_Level'].map(z_scores)\n\n# =================================================================================\n# FSN-BASED INVENTORY STRATEGY RECOMMENDATIONS\n# =================================================================================\ndef get_fsn_strategy(fsn_class):\n    \"\"\"\n    Inventory strategy based on FSN classification\n    \"\"\"\n    strategies = {\n        'F': {\n            'strategy': 'Frequent Reorder',\n            'review_frequency': 'Daily/Weekly',\n            'stock_policy': 'Maintain high stock levels',\n            'order_frequency': 'Frequent small orders',\n            'priority': 'High'\n        },\n        'S': {\n            'strategy': 'Periodic Review',\n            'review_frequency': 'Weekly/Monthly',\n            'stock_policy': 'Moderate stock levels',\n            'order_frequency': 'Regular scheduled orders',\n            'priority': 'Medium'\n        },\n        'N': {\n            'strategy': 'Order on Demand',\n            'review_frequency': 'Quarterly/Annually',\n            'stock_policy': 'Minimal or zero stock',\n            'order_frequency': 'Order when requested',\n            'priority': 'Low'\n        }\n    }\n    return strategies[fsn_class]\n\n# Apply FSN strategies\nfor key in ['strategy', 'review_frequency', 'stock_policy', 'order_frequency', 'priority']:\n    product_agg[f'FSN_{key}'] = product_agg['FSN_Class'].apply(\n        lambda x: get_fsn_strategy(x)[key]\n    )\n\n# =================================================================================\n# COMBINED ABC-FSN STRATEGIC RECOMMENDATIONS\n# =================================================================================\ndef get_combined_recommendation(row):\n    \"\"\"\n    Comprehensive inventory management recommendation\n    Based on ABC (criticality), FSN (velocity), and Demand Pattern\n    \"\"\"\n    abc = row['ABC_Class']\n    fsn = row['FSN_Class']\n    pattern = row['Demand_Pattern']\n    \n    # High Value (A) Products\n    if abc == 'A':\n        if fsn == 'F':\n            if pattern == 'Smooth':\n                return {\n                    'category': 'CRITICAL - STABLE',\n                    'recommendation': 'Implement continuous replenishment with VMI (Vendor Managed Inventory)',\n                    'safety_stock': 'High (30-45 days)',\n                    'reorder_method': 'Automated min-max system',\n                    'monitoring': 'Daily tracking with alerts'\n                }\n            else:  # Erratic\n                return {\n                    'category': 'CRITICAL - VARIABLE',\n                    'recommendation': 'Maintain substantial safety stock with dynamic reorder points',\n                    'safety_stock': 'Very High (45-60 days)',\n                    'reorder_method': 'Statistical safety stock calculation',\n                    'monitoring': 'Real-time monitoring required'\n                }\n        elif fsn == 'S':\n            return {\n                'category': 'HIGH VALUE - SLOW',\n                'recommendation': 'Reduce stock levels but ensure availability through supplier partnerships',\n                'safety_stock': 'Moderate (15-30 days)',\n                'reorder_method': 'Periodic review with lead time buffer',\n                'monitoring': 'Weekly review'\n            }\n        else:  # N\n            return {\n                'category': 'HIGH VALUE - NON-MOVING',\n                'recommendation': 'Evaluate for phase-out or move to make-to-order',\n                'safety_stock': 'Minimal (0-7 days)',\n                'reorder_method': 'Order on customer demand',\n                'monitoring': 'Monthly review for obsolescence'\n            }\n    \n    # Medium Value (B) Products\n    elif abc == 'B':\n        if fsn == 'F':\n            return {\n                'category': 'STANDARD - FAST',\n                'recommendation': 'Regular reorder with economic order quantity (EOQ) optimization',\n                'safety_stock': 'Moderate (20-30 days)',\n                'reorder_method': 'Fixed order quantity with reorder point',\n                'monitoring': 'Weekly monitoring'\n            }\n        elif fsn == 'S':\n            return {\n                'category': 'STANDARD - SLOW',\n                'recommendation': 'Periodic review system with flexible order quantities',\n                'safety_stock': 'Low-Moderate (10-20 days)',\n                'reorder_method': 'Periodic review (monthly)',\n                'monitoring': 'Bi-weekly review'\n            }\n        else:  # N\n            return {\n                'category': 'MEDIUM VALUE - NON-MOVING',\n                'recommendation': 'Consider discontinuation or clearance sale',\n                'safety_stock': 'Zero',\n                'reorder_method': 'No automatic reorder',\n                'monitoring': 'Quarterly review'\n            }\n    \n    # Low Value (C) Products\n    else:\n        if fsn == 'F':\n            return {\n                'category': 'LOW VALUE - FAST',\n                'recommendation': 'Bulk ordering with generous safety stock (low holding cost)',\n                'safety_stock': 'High (60-90 days)',\n                'reorder_method': 'Large lot sizes, infrequent orders',\n                'monitoring': 'Monthly monitoring'\n            }\n        elif fsn == 'S':\n            return {\n                'category': 'LOW VALUE - SLOW',\n                'recommendation': 'Minimal stock with basic reorder system',\n                'safety_stock': 'Low (7-15 days)',\n                'reorder_method': 'Two-bin system',\n                'monitoring': 'Monthly spot checks'\n            }\n        else:  # N\n            return {\n                'category': 'LOW VALUE - NON-MOVING',\n                'recommendation': 'Immediate phase-out or liquidation',\n                'safety_stock': 'Zero',\n                'reorder_method': 'No reorder',\n                'monitoring': 'Annual review for removal'\n            }\n\n# Apply comprehensive recommendations\nrecommendations = product_agg.apply(get_combined_recommendation, axis=1)\n\nproduct_agg['Recommendation_Category'] = recommendations.apply(lambda x: x['category'])\nproduct_agg['Recommendation'] = recommendations.apply(lambda x: x['recommendation'])\nproduct_agg['Safety_Stock_Days'] = recommendations.apply(lambda x: x['safety_stock'])\nproduct_agg['Reorder_Method'] = recommendations.apply(lambda x: x['reorder_method'])\nproduct_agg['Monitoring_Frequency'] = recommendations.apply(lambda x: x['monitoring'])\n\n# Strategic recommendation based on classification\ndef get_strategy(row):\n    abc = row['ABC_Class']\n    fsn = row['FSN_Class']\n    pattern = row['Demand_Pattern']\n    \n    # High value (A) products\n    if abc == 'A':\n        if fsn == 'F' and pattern == 'Smooth':\n            return 'CRITICAL: Continuous Replenishment - High Priority'\n        elif fsn == 'F' and pattern == 'Erratic':\n            return 'CRITICAL: Safety Stock Required - High Variability'\n        elif pattern == 'Lumpy':\n            return 'MONITOR: Forecast Carefully - Irregular High Value Demand'\n        elif fsn in ['S', 'N']:\n            return 'REVIEW: High Value but Slow - Optimize Stock Levels'\n        else:\n            return 'STRATEGIC: Maintain Service Level - Important Item'\n    \n    # Medium value (B) products\n    elif abc == 'B':\n        if pattern == 'Smooth':\n            return 'STANDARD: Regular Reorder - Predictable Demand'\n        elif pattern == 'Erratic':\n            return 'BUFFER STOCK: Moderate Safety Stock Required'\n        elif pattern == 'Intermittent':\n            return 'PERIODIC REVIEW: Monitor and Order as Needed'\n        else:  # Lumpy\n            return 'FLEXIBLE: Adjust Order Quantity Based on Forecast'\n    \n    # Low value (C) products\n    else:  # C\n        if fsn == 'F':\n            return 'OPTIMIZE: Bulk Ordering - High Volume Low Value'\n        elif fsn == 'N':\n            return 'PHASE OUT: Consider Discontinuation - Low Priority'\n        elif pattern == 'Lumpy':\n            return 'MINIMAL STOCK: Order on Demand Only'\n        else:\n            return 'BASIC: Simple Reorder System - Low Cost Items'\n\nproduct_agg['Strategy'] = product_agg.apply(get_strategy, axis=1)\n\n# =================================================================================\n# FINAL OUTPUT: TOP 10 PRODUCTS\n# =================================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"FINAL RESULTS: TOP 10 PRODUCTS BY DOLLAR USAGE\")\nprint(f\"{'='*80}\\n\")\n\n# Select columns for display\noutput_columns = [\n    'Product_ID', 'Product_Name', 'Category', 'Sub_Category',\n    'Dollar_Usage', 'Quantity', 'ABC_Class', 'FSN_Class', \n    'ADI', 'CV_Squared', 'Demand_Pattern', 'Combined_Class', 'Strategy'\n]\n\ntop_10 = product_agg[output_columns].head(10).copy()\n\n# Format for better display\ntop_10['Dollar_Usage'] = top_10['Dollar_Usage'].apply(lambda x: f\"${x:,.2f}\")\ntop_10['ADI'] = top_10['ADI'].apply(lambda x: f\"{x:.3f}\")\ntop_10['CV_Squared'] = top_10['CV_Squared'].apply(lambda x: f\"{x:.3f}\")\n\n# Display results\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', 40)\n\nprint(top_10.to_string(index=False))\n\n# =================================================================================\n# SUMMARY STATISTICS\n# =================================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"CLASSIFICATION SUMMARY STATISTICS\")\nprint(f\"{'='*80}\\n\")\n\n# ABC-FSN Cross-tabulation\nprint(\"ABC-FSN Matrix:\")\nabc_fsn_matrix = pd.crosstab(product_agg['ABC_Class'], product_agg['FSN_Class'])\nprint(abc_fsn_matrix)\n\nprint(f\"\\n{'-'*80}\\n\")\n\n# ABC-DemandPattern Cross-tabulation\nprint(\"ABC-Demand Pattern Matrix:\")\nabc_pattern_matrix = pd.crosstab(product_agg['ABC_Class'], product_agg['Demand_Pattern'])\nprint(abc_pattern_matrix)\n\nprint(f\"\\n{'-'*80}\\n\")\n\n# FSN-DemandPattern Cross-tabulation\nprint(\"FSN-Demand Pattern Matrix:\")\nfsn_pattern_matrix = pd.crosstab(product_agg['FSN_Class'], product_agg['Demand_Pattern'])\nprint(fsn_pattern_matrix)\n\nprint(f\"\\n{'-'*80}\\n\")\n\n# =================================================================================\n# SERVICE LEVEL DISTRIBUTION\n# =================================================================================\nprint(\"SERVICE LEVEL DISTRIBUTION:\")\nservice_level_dist = product_agg.groupby('ABC_Class').agg({\n    'Product_ID': 'count',\n    'Service_Level': 'first',\n    'Dollar_Usage': 'sum'\n}).round(2)\nservice_level_dist.columns = ['Product_Count', 'Service_Level_Target', 'Total_Dollar_Usage']\nprint(service_level_dist)\n\nprint(f\"\\n{'-'*80}\\n\")\n\n# =================================================================================\n# FSN STRATEGY DISTRIBUTION\n# =================================================================================\nprint(\"FSN INVENTORY STRATEGY DISTRIBUTION:\")\nfsn_strategy = product_agg.groupby('FSN_Class').agg({\n    'Product_ID': 'count',\n    'FSN_strategy': 'first',\n    'FSN_priority': 'first'\n}).round(2)\nfsn_strategy.columns = ['Product_Count', 'Strategy', 'Priority']\nprint(fsn_strategy)\n\nprint(f\"\\n{'-'*80}\\n\")\n\n# =================================================================================\n# RECOMMENDATION CATEGORY SUMMARY\n# =================================================================================\nprint(\"RECOMMENDATION CATEGORY SUMMARY:\")\nrec_summary = product_agg['Recommendation_Category'].value_counts().sort_index()\nprint(\"\\nProduct Distribution by Recommendation Category:\")\nfor category, count in rec_summary.items():\n    pct = (count / len(product_agg)) * 100\n    print(f\"  {category}: {count} products ({pct:.1f}%)\")\n\nprint(f\"\\n{'-'*80}\\n\")\n\n# =================================================================================\n# KEY INSIGHTS & ACTION ITEMS\n# =================================================================================\nprint(\"KEY INSIGHTS & ACTION ITEMS:\")\nprint(f\"\\n1. HIGH PRIORITY ITEMS (Class A):\")\nclass_a_count = len(product_agg[product_agg['ABC_Class'] == 'A'])\nclass_a_value = product_agg[product_agg['ABC_Class'] == 'A']['Dollar_Usage'].sum()\nclass_a_pct = (class_a_value / product_agg['Dollar_Usage'].sum()) * 100\nprint(f\"   • {class_a_count} products ({class_a_count/len(product_agg)*100:.1f}%)\")\nprint(f\"   • ${class_a_value:,.2f} total value ({class_a_pct:.1f}% of total)\")\nprint(f\"   • Target Service Level: 99%\")\nprint(f\"   • Requires daily monitoring and automated replenishment\")\n\nprint(f\"\\n2. FAST MOVING ITEMS (Class F):\")\nclass_f_count = len(product_agg[product_agg['FSN_Class'] == 'F'])\nfast_critical = len(product_agg[(product_agg['ABC_Class'] == 'A') & (product_agg['FSN_Class'] == 'F')])\nprint(f\"   • {class_f_count} products ({class_f_count/len(product_agg)*100:.1f}%)\")\nprint(f\"   • {fast_critical} are CRITICAL (A-F combination)\")\nprint(f\"   • Implement frequent reorder systems\")\n\nprint(f\"\\n3. NON-MOVING ITEMS (Class N):\")\nclass_n_count = len(product_agg[product_agg['FSN_Class'] == 'N'])\nclass_n_value = product_agg[product_agg['FSN_Class'] == 'N']['Dollar_Usage'].sum()\nprint(f\"   • {class_n_count} products ({class_n_count/len(product_agg)*100:.1f}%)\")\nprint(f\"   • ${class_n_value:,.2f} tied up in slow inventory\")\nprint(f\"   • Review for discontinuation or liquidation\")\n\nprint(f\"\\n4. DEMAND PATTERN INSIGHTS:\")\nlumpy_count = len(product_agg[product_agg['Demand_Pattern'] == 'Lumpy'])\nerratic_count = len(product_agg[product_agg['Demand_Pattern'] == 'Erratic'])\nprint(f\"   • {lumpy_count} Lumpy items - Require higher safety stock\")\nprint(f\"   • {erratic_count} Erratic items - Need dynamic reorder points\")\nprint(f\"   • Focus forecasting efforts on these categories\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ SECTION 1: CLASSIFICATION ANALYSIS COMPLETE\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:04:10.059425Z","iopub.execute_input":"2026-02-09T08:04:10.059985Z","iopub.status.idle":"2026-02-09T08:04:10.812614Z","shell.execute_reply.started":"2026-02-09T08:04:10.05995Z","shell.execute_reply":"2026-02-09T08:04:10.811603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2) DEMAND PATTERN VISUALIZATION ","metadata":{}},{"cell_type":"markdown","source":"Section 2 focuses on visualizing demand patterns through time series analysis and monthly product popularity to illustrate the behavioral differences between smooth, erratic, intermittent, and lumpy demand.\nThis section enhances interpretability by combining graphical demand trends with monthly top-product analysis, supporting demand characterization and inventory policy validation.","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# SECTION 2: DEMAND PATTERN VISUALIZATION \n# Time Series Graphs + Monthly Popular Products Analysis\n# =================================================================================\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib.gridspec import GridSpec\nimport pandas as pd\nimport numpy as np\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DEMAND PATTERN VISUALIZATION - ENHANCED VERSION\")\nprint(\"=\"*80)\n\n# =================================================================================\n# STEP 1: SELECT SAMPLE PRODUCTS (MINIMUM 2 ORDERS)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 1: Selecting Sample Products (Min 2 Orders)\")\nprint(f\"{'-'*80}\")\n\n# Filter products with at least 2 orders\nproduct_agg_filtered = product_agg[product_agg['Order_Count'] >= 2].copy()\n\nprint(f\"✓ Total products with ≥2 orders: {len(product_agg_filtered)}\")\n\n# Select 3 representative products from each demand pattern\nsample_products = {}\nfor pattern in ['Smooth', 'Erratic', 'Intermittent', 'Lumpy']:\n    pattern_products = product_agg_filtered[product_agg_filtered['Demand_Pattern'] == pattern]\n    \n    if len(pattern_products) >= 3:\n        # Select products with highest dollar usage for better examples\n        samples = pattern_products.nlargest(3, 'Dollar_Usage')['Product_ID'].tolist()\n    else:\n        # If less than 3, take all available\n        samples = pattern_products['Product_ID'].tolist()\n    \n    sample_products[pattern] = samples\n    print(f\"✓ {pattern}: {len(samples)} products selected\")\n\n# =================================================================================\n# STEP 2: PREPARE TIME SERIES DATA\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2: Preparing Time Series Data\")\nprint(f\"{'-'*80}\")\n\n# Function to get time series data for a product\ndef get_product_timeseries(product_id, df):\n    \"\"\"Extract time series data for a specific product\"\"\"\n    product_data = df[df['Product ID'] == product_id].copy()\n    product_data = product_data.sort_values('Order Date')\n    \n    # Group by date and sum sales\n    daily_data = product_data.groupby('Order Date').agg({\n        'Sales': 'sum',\n        'Product Name': 'first'\n    }).reset_index()\n    \n    return daily_data\n\nprint(f\"✓ Time series extraction function ready\")\n\n# =================================================================================\n# STEP 2.5: MONTHLY POPULAR PRODUCTS ANALYSIS (NEW!)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2.5: Analyzing Monthly Popular Products by Demand Pattern\")\nprint(f\"{'-'*80}\")\n\n# Add year-month column to original dataframe\ndf['Year_Month'] = df['Order Date'].dt.to_period('M')\n\n# Function to get top products by month and pattern\ndef get_monthly_top_products(df, product_agg, pattern, top_n=5):\n    \"\"\"Get top products for each month filtered by demand pattern\"\"\"\n    \n    # Get product IDs for this pattern\n    pattern_products = product_agg[product_agg['Demand_Pattern'] == pattern]['Product_ID'].tolist()\n    \n    # Filter dataframe for this pattern's products\n    pattern_df = df[df['Product ID'].isin(pattern_products)].copy()\n    \n    # Group by month and product\n    monthly_products = pattern_df.groupby(['Year_Month', 'Product ID', 'Product Name']).agg({\n        'Sales': ['sum', 'count']\n    }).reset_index()\n    \n    # Flatten column names\n    monthly_products.columns = ['Year_Month', 'Product_ID', 'Product_Name', 'Sales', 'Order_Count']\n    \n    # Get top N products for each month\n    top_monthly = monthly_products.groupby('Year_Month', group_keys=False).apply(\n        lambda x: x.nlargest(top_n, 'Sales')\n    ).reset_index(drop=True)\n    \n    return top_monthly\n\n# Analyze for each pattern\nmonthly_analysis = {}\nfor pattern in ['Smooth', 'Erratic', 'Intermittent', 'Lumpy']:\n    monthly_analysis[pattern] = get_monthly_top_products(df, product_agg, pattern, top_n=5)\n    print(f\"✓ {pattern} pattern: Monthly analysis complete\")\n\n# Create summary report\nprint(f\"\\n{'-'*80}\")\nprint(\"MONTHLY POPULAR PRODUCTS SUMMARY\")\nprint(f\"{'-'*80}\")\n\nfor pattern in ['Smooth', 'Erratic', 'Intermittent', 'Lumpy']:\n    print(f\"\\n{'='*80}\")\n    print(f\"{pattern.upper()} DEMAND PATTERN - TOP PRODUCTS BY MONTH\")\n    print(f\"{'='*80}\")\n    \n    pattern_data = monthly_analysis[pattern]\n    \n    if len(pattern_data) == 0:\n        print(f\"  No data available for {pattern} pattern\")\n        continue\n    \n    # Get last 6 months for display\n    all_months = sorted(pattern_data['Year_Month'].unique())\n    recent_months = all_months[-6:] if len(all_months) >= 6 else all_months\n    \n    for month in recent_months:\n        month_data = pattern_data[pattern_data['Year_Month'] == month]\n        \n        print(f\"\\n {month} (Top 5 Products):\")\n        print(f\"{'-'*80}\")\n        \n        for idx, row in enumerate(month_data.itertuples(), 1):\n            print(f\"  {idx}. {row.Product_Name[:50]}\")\n            print(f\"     Product ID: {row.Product_ID}\")\n            print(f\"     Sales: ${row.Sales:,.2f} | Orders: {row.Order_Count:.0f}\")\n            print()\n\n# =================================================================================\n# STEP 3: CREATE VISUALIZATION FOR EACH DEMAND PATTERN\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 3: Creating Demand Pattern Visualizations\")\nprint(f\"{'-'*80}\")\n\n# Set up the plot style\nplt.style.use('seaborn-v0_8-darkgrid')\ncolors = {\n    'Smooth': '#2ecc71',      # Green\n    'Erratic': '#e74c3c',     # Red\n    'Intermittent': '#3498db', # Blue\n    'Lumpy': '#f39c12'        # Orange\n}\n\n# Create figure with subplots for all patterns\nfig = plt.figure(figsize=(20, 24))\ngs = GridSpec(4, 3, figure=fig, hspace=0.4, wspace=0.3)\n\npattern_idx = 0\nfor pattern in ['Smooth', 'Erratic', 'Intermittent', 'Lumpy']:\n    print(f\"\\n  Creating {pattern} pattern graphs...\")\n    \n    products = sample_products[pattern]\n    \n    # Skip if no products\n    if len(products) == 0:\n        print(f\"    ⚠ No products available for {pattern} pattern\")\n        pattern_idx += 1\n        continue\n    \n    for prod_idx, product_id in enumerate(products):\n        # Get time series data\n        ts_data = get_product_timeseries(product_id, df)\n        product_name = ts_data['Product Name'].iloc[0]\n        \n        # Get product metrics from aggregation\n        prod_metrics = product_agg[product_agg['Product_ID'] == product_id].iloc[0]\n        \n        # Create subplot\n        ax = fig.add_subplot(gs[pattern_idx, prod_idx])\n        \n        # Plot time series\n        ax.plot(ts_data['Order Date'], ts_data['Sales'], \n                marker='o', markersize=4, linewidth=1.5, \n                color=colors[pattern], alpha=0.7)\n        \n        # Formatting\n        ax.set_title(f\"{pattern} Pattern: {product_name[:30]}\\n\"\n                    f\"Product ID: {product_id}\", \n                    fontsize=10, fontweight='bold')\n        ax.set_xlabel('Date', fontsize=9)\n        ax.set_ylabel('Sales ($)', fontsize=9)\n        ax.grid(True, alpha=0.3)\n        \n        metrics_text = (\n            f\"ADI: {prod_metrics['ADI']:.2f}\\n\"\n            f\"CV²: {prod_metrics['CV_Squared']:.2f}\\n\"\n            f\"ABC: {prod_metrics['ABC_Class']}\\n\"\n            f\"FSN: {prod_metrics['FSN_Class']}\"\n        )\n        ax.text(0.02, 0.98, metrics_text,\n               transform=ax.transAxes,\n               fontsize=8,\n               verticalalignment='top',\n               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n        \n        # Format x-axis dates\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n        \n        # Add horizontal line for mean\n        mean_sales = ts_data['Sales'].mean()\n        ax.axhline(y=mean_sales, color='gray', linestyle='--', \n                  linewidth=1, alpha=0.5, label=f'Mean: ${mean_sales:.2f}')\n        ax.legend(fontsize=8, loc='upper right')\n        \n        print(f\"    ✓ Product {prod_idx+1}: {product_name[:30]}\")\n    \n    pattern_idx += 1\n\n# Add main title\nfig.suptitle('Demand Pattern Analysis - Time Series Visualization\\n'\n            'Sample Products from Each Pattern Category (Min 2 Orders)',\n            fontsize=16, fontweight='bold', y=0.995)\n\nplt.tight_layout()\nplt.savefig('demand_pattern_visualization.png', dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Visualization saved as: demand_pattern_visualization.png\")\nplt.show()\n\n# =================================================================================\n# STEP 4: CREATE PATTERN COMPARISON CHART\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 4: Creating Pattern Comparison Chart\")\nprint(f\"{'-'*80}\")\n\nfig2, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\nfor idx, pattern in enumerate(['Smooth', 'Erratic', 'Intermittent', 'Lumpy']):\n    ax = axes[idx]\n    products = sample_products[pattern]\n    \n    if len(products) == 0:\n        ax.text(0.5, 0.5, f'No {pattern} products available', \n                ha='center', va='center', fontsize=12)\n        ax.set_title(f\"{pattern} Demand Pattern\", fontsize=12, fontweight='bold')\n        continue\n    \n    for prod_idx, product_id in enumerate(products):\n        ts_data = get_product_timeseries(product_id, df)\n        product_name = ts_data['Product Name'].iloc[0]\n        \n        # Plot with different colors for each product\n        ax.plot(ts_data['Order Date'], ts_data['Sales'], \n               marker='o', markersize=3, linewidth=1.5, \n               alpha=0.7, label=f\"{product_name[:20]}...\")\n    \n    ax.set_title(f\"{pattern} Demand Pattern\\n\"\n                f\"Characteristics: ADI {'>' if pattern in ['Intermittent', 'Lumpy'] else '≤'} 1.32, \"\n                f\"CV² {'>' if pattern in ['Erratic', 'Lumpy'] else '≤'} 0.49\",\n                fontsize=12, fontweight='bold')\n    ax.set_xlabel('Date', fontsize=10)\n    ax.set_ylabel('Sales ($)', fontsize=10)\n    ax.legend(fontsize=8, loc='best')\n    ax.grid(True, alpha=0.3)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n\nplt.tight_layout()\nplt.savefig('demand_pattern_comparison.png', dpi=300, bbox_inches='tight')\nprint(f\"✓ Comparison chart saved as: demand_pattern_comparison.png\")\nplt.show()\n\n# =================================================================================\n# STEP 5: CREATE MONTHLY HEATMAP FOR EACH PATTERN (NEW!)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 5: Creating Monthly Product Popularity Heatmap\")\nprint(f\"{'-'*80}\")\n\nfig3, axes = plt.subplots(2, 2, figsize=(20, 16))\naxes = axes.flatten()\n\nfor idx, pattern in enumerate(['Smooth', 'Erratic', 'Intermittent', 'Lumpy']):\n    ax = axes[idx]\n    \n    # Get monthly data for this pattern\n    pattern_monthly = monthly_analysis[pattern].copy()\n    \n    if len(pattern_monthly) == 0:\n        ax.text(0.5, 0.5, f'No {pattern} products available', \n                ha='center', va='center', fontsize=12)\n        ax.set_title(f'{pattern} Pattern - Monthly Product Popularity',\n                    fontsize=12, fontweight='bold')\n        continue\n    \n    # Pivot to create heatmap data (top 10 products overall)\n    top_products = pattern_monthly.groupby('Product_Name')['Sales'].sum().nlargest(10).index\n    heatmap_data = pattern_monthly[pattern_monthly['Product_Name'].isin(top_products)]\n    \n    # Create pivot table\n    pivot_data = heatmap_data.pivot_table(\n        values='Sales',\n        index='Product_Name',\n        columns='Year_Month',\n        aggfunc='sum',\n        fill_value=0\n    )\n    \n    if pivot_data.empty:\n        ax.text(0.5, 0.5, f'Insufficient data for {pattern}', \n                ha='center', va='center', fontsize=12)\n        continue\n    \n    # Plot heatmap\n    im = ax.imshow(pivot_data.values, cmap='YlOrRd', aspect='auto')\n    \n    # Set ticks\n    ax.set_xticks(np.arange(len(pivot_data.columns)))\n    ax.set_yticks(np.arange(len(pivot_data.index)))\n    ax.set_xticklabels([str(col) for col in pivot_data.columns], rotation=45, ha='right')\n    ax.set_yticklabels([name[:30] for name in pivot_data.index], fontsize=8)\n    \n    # Add colorbar\n    cbar = plt.colorbar(im, ax=ax)\n    cbar.set_label('Sales ($)', rotation=270, labelpad=15)\n    \n    # Title\n    ax.set_title(f'{pattern} Pattern - Monthly Product Popularity\\n(Top 10 Products)',\n                fontsize=12, fontweight='bold')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Product Name')\n    \n    print(f\"✓ {pattern} heatmap created\")\n\nplt.tight_layout()\nplt.savefig('monthly_product_heatmap.png', dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Heatmap saved as: monthly_product_heatmap.png\")\nplt.show()\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ SECTION 2: DEMAND PATTERN VISUALIZATION COMPLETE (ENHANCED)\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:05:40.199355Z","iopub.execute_input":"2026-02-09T08:05:40.200523Z","iopub.status.idle":"2026-02-09T08:05:54.680039Z","shell.execute_reply.started":"2026-02-09T08:05:40.200475Z","shell.execute_reply":"2026-02-09T08:05:54.679002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3) PRODUCT PERFORMANCE ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"Section 3 focuses on evaluating product performance through category, sub category, and product level sales analysis to assess market share, portfolio structure, and revenue contribution.\nThis section integrates ABC–FSN classification and a BCG-style market position matrix to support strategic product prioritization and portfolio decision making.","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# SECTION 3: PRODUCT PERFORMANCE ANALYSIS\n# Sales Performance, Market Position, and Portfolio Analysis\n# =================================================================================\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 3: PRODUCT PERFORMANCE ANALYSIS\")\nprint(\"=\"*80)\n\n# =================================================================================\n# STEP 1: CATEGORY PERFORMANCE ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 1: Category Performance Analysis\")\nprint(f\"{'-'*80}\")\n\n# Aggregate by Category\ncategory_perf = df.groupby('Category').agg({\n    'Sales': ['sum', 'mean', 'count'],\n    'Product ID': 'nunique'\n}).reset_index()\n\ncategory_perf.columns = ['Category', 'Total_Sales', 'Avg_Transaction', \n                          'Total_Transactions', 'Unique_Products']\n\ncategory_perf = category_perf.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nCategory Performance Summary:\")\nprint(category_perf.to_string(index=False))\n\n# Calculate market share\ntotal_sales = category_perf['Total_Sales'].sum()\ncategory_perf['Market_Share_%'] = (category_perf['Total_Sales'] / total_sales) * 100\n\nprint(f\"\\nMarket Share by Category:\")\nfor idx, row in category_perf.iterrows():\n    print(f\"  {row['Category']}: {row['Market_Share_%']:.2f}%\")\n\n# =================================================================================\n# STEP 2: SUB-CATEGORY PERFORMANCE ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2: Sub-Category Performance Analysis\")\nprint(f\"{'-'*80}\")\n\n# Aggregate by Sub-Category\nsubcat_perf = df.groupby(['Category', 'Sub-Category']).agg({\n    'Sales': ['sum', 'mean', 'count'],\n    'Product ID': 'nunique'\n}).reset_index()\n\nsubcat_perf.columns = ['Category', 'Sub_Category', 'Total_Sales', \n                        'Avg_Transaction', 'Total_Transactions', 'Unique_Products']\n\nsubcat_perf = subcat_perf.sort_values('Total_Sales', ascending=False)\n\n# Top and Bottom 10\nprint(\"\\nTop 10 Sub-Categories by Sales:\")\nprint(subcat_perf.head(10)[['Category', 'Sub_Category', 'Total_Sales', \n                              'Total_Transactions']].to_string(index=False))\n\nprint(\"\\nBottom 10 Sub-Categories by Sales:\")\nprint(subcat_perf.tail(10)[['Category', 'Sub_Category', 'Total_Sales', \n                              'Total_Transactions']].to_string(index=False))\n\n# =================================================================================\n# STEP 3: PRODUCT-LEVEL PERFORMANCE\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 3: Product-Level Performance Analysis\")\nprint(f\"{'-'*80}\")\n\n# Get product performance from existing product_agg\nproduct_perf = product_agg[['Product_ID', 'Product_Name', 'Category', \n                             'Sub_Category', 'Dollar_Usage', 'Order_Count',\n                             'ABC_Class', 'FSN_Class', 'Demand_Pattern']].copy()\n\nproduct_perf = product_perf.rename(columns={\n    'Dollar_Usage': 'Total_Sales',\n    'Order_Count': 'Transaction_Count'\n})\n\nproduct_perf = product_perf.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nTop 20 Products by Sales:\")\nprint(product_perf.head(20)[['Product_ID', 'Product_Name', 'Category', \n                              'Total_Sales', 'Transaction_Count']].to_string(index=False))\n\nprint(\"\\nBottom 20 Products by Sales:\")\nprint(product_perf.tail(20)[['Product_ID', 'Product_Name', 'Category', \n                              'Total_Sales', 'Transaction_Count']].to_string(index=False))\n\n# =================================================================================\n# STEP 4: MARKET POSITION ANALYSIS (BCG MATRIX APPROACH)\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 4: Market Position Analysis (Growth vs Market Share)\")\nprint(f\"{'-'*80}\")\n\n# Calculate growth rate (using transaction count as proxy)\n# Calculate median values for positioning\nmedian_sales = product_perf['Total_Sales'].median()\nmedian_transactions = product_perf['Transaction_Count'].median()\n\n# Classify products into BCG-style quadrants\ndef classify_market_position(row):\n    \"\"\"\n    Classify products based on Sales (Market Share proxy) and Transactions (Growth proxy)\n    Stars: High Sales + High Transactions\n    Cash Cows: High Sales + Low Transactions\n    Question Marks: Low Sales + High Transactions\n    Dogs: Low Sales + Low Transactions\n    \"\"\"\n    if row['Total_Sales'] >= median_sales and row['Transaction_Count'] >= median_transactions:\n        return 'Stars'\n    elif row['Total_Sales'] >= median_sales and row['Transaction_Count'] < median_transactions:\n        return 'Cash Cows'\n    elif row['Total_Sales'] < median_sales and row['Transaction_Count'] >= median_transactions:\n        return 'Question Marks'\n    else:\n        return 'Dogs'\n\nproduct_perf['Market_Position'] = product_perf.apply(classify_market_position, axis=1)\n\n# Summary by position\nposition_summary = product_perf.groupby('Market_Position').agg({\n    'Product_ID': 'count',\n    'Total_Sales': 'sum',\n    'Transaction_Count': 'sum'\n}).round(2)\n\nposition_summary.columns = ['Product_Count', 'Total_Sales', 'Total_Transactions']\nposition_summary['Avg_Sales_Per_Product'] = (position_summary['Total_Sales'] / \n                                               position_summary['Product_Count']).round(2)\n\nprint(\"\\nMarket Position Summary:\")\nprint(position_summary)\n\n# Strategic recommendations by position\nprint(\"\\nStrategic Recommendations by Market Position:\")\nprint(\"  • Stars: Invest & Grow - High performers with strong momentum\")\nprint(\"  • Cash Cows: Maintain & Harvest - Stable revenue generators\")\nprint(\"  • Question Marks: Selective Investment - Evaluate potential\")\nprint(\"  • Dogs: Review for Phase-Out - Low priority items\")\n\n# =================================================================================\n# STEP 5: CREATE COMPREHENSIVE VISUALIZATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 5: Creating Performance Visualizations\")\nprint(f\"{'-'*80}\")\n\n# Set style\nplt.style.use('seaborn-v0_8-whitegrid')\ncolors_palette = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n\n# Create main figure with multiple subplots\nfig = plt.figure(figsize=(20, 16))\ngs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n\n# ---------------------------------------------------------------------------------\n# CHART 1: Category Market Share (Pie Chart)\n# ---------------------------------------------------------------------------------\nax1 = fig.add_subplot(gs[0, 0])\ncolors_cat = ['#3498db', '#e74c3c', '#2ecc71']\nwedges, texts, autotexts = ax1.pie(category_perf['Total_Sales'], \n                                     labels=category_perf['Category'],\n                                     autopct='%1.1f%%',\n                                     colors=colors_cat,\n                                     startangle=90,\n                                     explode=(0.05, 0.05, 0.05))\n\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(10)\n    autotext.set_weight('bold')\n\nax1.set_title('Market Share by Category', fontsize=12, fontweight='bold', pad=20)\nprint(\"  ✓ Chart 1: Category Market Share created\")\n\n# ---------------------------------------------------------------------------------\n# CHART 2: Category Sales Comparison (Bar Chart)\n# ---------------------------------------------------------------------------------\nax2 = fig.add_subplot(gs[0, 1])\nbars = ax2.bar(category_perf['Category'], category_perf['Total_Sales'], \n               color=colors_cat, alpha=0.7, edgecolor='black')\n\n# Add value labels on bars\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f'${height:,.0f}',\n            ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nax2.set_title('Total Sales by Category', fontsize=12, fontweight='bold')\nax2.set_ylabel('Total Sales ($)', fontsize=10)\nax2.set_xlabel('Category', fontsize=10)\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax2.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 2: Category Sales Comparison created\")\n\n# ---------------------------------------------------------------------------------\n# CHART 3: Category Product Count\n# ---------------------------------------------------------------------------------\nax3 = fig.add_subplot(gs[0, 2])\nbars = ax3.bar(category_perf['Category'], category_perf['Unique_Products'], \n               color=colors_cat, alpha=0.7, edgecolor='black')\n\nfor bar in bars:\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n            f'{int(height)}',\n            ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nax3.set_title('Number of Unique Products by Category', fontsize=12, fontweight='bold')\nax3.set_ylabel('Product Count', fontsize=10)\nax3.set_xlabel('Category', fontsize=10)\nax3.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 3: Product Count by Category created\")\n\n# ---------------------------------------------------------------------------------\n# CHART 4: Top 10 Sub-Categories (Horizontal Bar)\n# ---------------------------------------------------------------------------------\nax4 = fig.add_subplot(gs[1, :])\ntop_subcat = subcat_perf.head(10).sort_values('Total_Sales')\n\n# Color by category\ncat_colors = {'Technology': '#3498db', 'Furniture': '#e74c3c', 'Office Supplies': '#2ecc71'}\nbar_colors = [cat_colors[cat] for cat in top_subcat['Category']]\n\nbars = ax4.barh(range(len(top_subcat)), top_subcat['Total_Sales'], \n                color=bar_colors, alpha=0.7, edgecolor='black')\n\nax4.set_yticks(range(len(top_subcat)))\nax4.set_yticklabels([f\"{row['Sub_Category']} ({row['Category']})\" \n                      for _, row in top_subcat.iterrows()], fontsize=9)\n\n# Add value labels\nfor i, (idx, row) in enumerate(top_subcat.iterrows()):\n    ax4.text(row['Total_Sales'], i, f\" ${row['Total_Sales']:,.0f}\", \n            va='center', fontsize=8, fontweight='bold')\n\nax4.set_title('Top 10 Sub-Categories by Total Sales', fontsize=12, fontweight='bold')\nax4.set_xlabel('Total Sales ($)', fontsize=10)\nax4.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax4.grid(axis='x', alpha=0.3)\n\n# Add legend for categories\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=cat_colors[cat], label=cat, alpha=0.7) \n                   for cat in cat_colors.keys()]\nax4.legend(handles=legend_elements, loc='lower right', fontsize=9)\n\nprint(\"  ✓ Chart 4: Top Sub-Categories created\")\n\n# ---------------------------------------------------------------------------------\n# CHART 5: Market Position Matrix (BCG Style)\n# ---------------------------------------------------------------------------------\nax5 = fig.add_subplot(gs[2, 0:2])\n\n# Sample products for clearer visualization (top 100 by sales)\ntop_products = product_perf.head(100).copy()\n\n# Color mapping for market positions\nposition_colors = {\n    'Stars': '#2ecc71',           # Green\n    'Cash Cows': '#3498db',       # Blue\n    'Question Marks': '#f39c12',  # Orange\n    'Dogs': '#95a5a6'             # Gray\n}\n\n# Plot scatter\nfor position in ['Dogs', 'Cash Cows', 'Question Marks', 'Stars']:\n    pos_data = top_products[top_products['Market_Position'] == position]\n    ax5.scatter(pos_data['Transaction_Count'], pos_data['Total_Sales'],\n               c=position_colors[position], label=position, \n               s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\n\n# Add median lines\nax5.axhline(y=median_sales, color='red', linestyle='--', linewidth=1, alpha=0.5)\nax5.axvline(x=median_transactions, color='red', linestyle='--', linewidth=1, alpha=0.5)\n\n# Add quadrant labels\nax5.text(median_transactions * 1.5, median_sales * 1.5, 'STARS', \n        fontsize=14, alpha=0.3, weight='bold', ha='center')\nax5.text(median_transactions * 0.3, median_sales * 1.5, 'CASH COWS', \n        fontsize=14, alpha=0.3, weight='bold', ha='center')\nax5.text(median_transactions * 1.5, median_sales * 0.3, 'QUESTION MARKS', \n        fontsize=14, alpha=0.3, weight='bold', ha='center')\nax5.text(median_transactions * 0.3, median_sales * 0.3, 'DOGS', \n        fontsize=14, alpha=0.3, weight='bold', ha='center')\n\nax5.set_title('Market Position Matrix - Top 100 Products\\n(Transaction Frequency vs Sales Performance)', \n             fontsize=12, fontweight='bold')\nax5.set_xlabel('Transaction Count (Frequency)', fontsize=10)\nax5.set_ylabel('Total Sales ($)', fontsize=10)\nax5.legend(loc='upper right', fontsize=9)\nax5.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 5: Market Position Matrix created\")\n\n# ---------------------------------------------------------------------------------\n# CHART 6: Market Position Distribution (Pie Chart)\n# ---------------------------------------------------------------------------------\nax6 = fig.add_subplot(gs[2, 2])\n\nposition_counts = product_perf['Market_Position'].value_counts()\ncolors_pos = [position_colors[pos] for pos in position_counts.index]\n\nwedges, texts, autotexts = ax6.pie(position_counts.values, \n                                     labels=position_counts.index,\n                                     autopct='%1.1f%%',\n                                     colors=colors_pos,\n                                     startangle=90)\n\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(9)\n    autotext.set_weight('bold')\n\nax6.set_title('Product Distribution by\\nMarket Position', fontsize=12, fontweight='bold')\nprint(\"  ✓ Chart 6: Market Position Distribution created\")\n\n# Add main title\nfig.suptitle('Product Performance & Market Analysis Dashboard', \n            fontsize=16, fontweight='bold', y=0.995)\n\nplt.tight_layout()\nplt.savefig('product_performance_analysis.png', dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Visualization saved as: product_performance_analysis.png\")\nplt.show()\n\n# =================================================================================\n# STEP 6: PERFORMANCE SUMMARY BY ABC-FSN-POSITION\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 6: Combined Performance Summary\")\nprint(f\"{'-'*80}\")\n\n# Cross-tabulation: ABC vs Market Position\nprint(\"\\nABC Class vs Market Position:\")\nabc_position = pd.crosstab(product_perf['ABC_Class'], \n                           product_perf['Market_Position'],\n                           margins=True)\nprint(abc_position)\n\nprint(\"\\nFSN Class vs Market Position:\")\nfsn_position = pd.crosstab(product_perf['FSN_Class'], \n                           product_perf['Market_Position'],\n                           margins=True)\nprint(fsn_position)\n\n# Strategic Portfolio Analysis\nprint(f\"\\n{'-'*80}\")\nprint(\"STRATEGIC PORTFOLIO INSIGHTS\")\nprint(f\"{'-'*80}\")\n\n# Key metrics\ntotal_products = len(product_perf)\nstars_count = len(product_perf[product_perf['Market_Position'] == 'Stars'])\ncash_cows_count = len(product_perf[product_perf['Market_Position'] == 'Cash Cows'])\ndogs_count = len(product_perf[product_perf['Market_Position'] == 'Dogs'])\n\nstars_sales = product_perf[product_perf['Market_Position'] == 'Stars']['Total_Sales'].sum()\ntotal_sales_all = product_perf['Total_Sales'].sum()\n\nprint(f\"\\n1. PORTFOLIO COMPOSITION:\")\nprint(f\"   • Total Products: {total_products}\")\nprint(f\"   • Stars: {stars_count} ({stars_count/total_products*100:.1f}%)\")\nprint(f\"   • Cash Cows: {cash_cows_count} ({cash_cows_count/total_products*100:.1f}%)\")\nprint(f\"   • Dogs: {dogs_count} ({dogs_count/total_products*100:.1f}%)\")\n\nprint(f\"\\n2. REVENUE CONTRIBUTION:\")\nprint(f\"   • Stars contribute: ${stars_sales:,.2f} ({stars_sales/total_sales_all*100:.1f}% of total)\")\n\nprint(f\"\\n3. CRITICAL ITEMS (Class A Stars):\")\ncritical_stars = product_perf[(product_perf['ABC_Class'] == 'A') & \n                               (product_perf['Market_Position'] == 'Stars')]\nprint(f\"   • {len(critical_stars)} products\")\nprint(f\"   • These are your TOP PRIORITY items - invest heavily\")\n\nprint(f\"\\n4. UNDERPERFORMERS (Class C Dogs):\")\nunderperformers = product_perf[(product_perf['ABC_Class'] == 'C') & \n                                (product_perf['Market_Position'] == 'Dogs')]\nprint(f\"   • {len(underperformers)} products\")\nprint(f\"   • Consider phase-out or discontinuation\")\n\nprint(f\"\\n5. OPPORTUNITIES (Class A/B Question Marks):\")\nopportunities = product_perf[(product_perf['ABC_Class'].isin(['A', 'B'])) & \n                              (product_perf['Market_Position'] == 'Question Marks')]\nprint(f\"   • {len(opportunities)} products\")\nprint(f\"   • High potential - need marketing/promotion push\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ SECTION 3: PRODUCT PERFORMANCE ANALYSIS COMPLETE\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:07:42.246142Z","iopub.execute_input":"2026-02-09T08:07:42.246649Z","iopub.status.idle":"2026-02-09T08:07:46.050283Z","shell.execute_reply.started":"2026-02-09T08:07:42.246608Z","shell.execute_reply":"2026-02-09T08:07:46.049079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4) CUSTOMER SEGMENTATION USING RFM, K-MEANS CLUSTERING, CLV","metadata":{}},{"cell_type":"markdown","source":"Section 4 focuses on identifying distinct customer segments by analyzing Recency, Frequency, and Monetary (RFM) behavior, applying K-Means clustering, and estimating Customer Lifetime Value (CLV) to inform targeted strategies.","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# SECTION 4: CUSTOMER SEGMENTATION USING RFM, K-MEANS CLUSTERING, CLV\n# RFM Analysis, K-Means Clustering, CLV Estimation, and Strategic Recommendations\n# =================================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nfrom datetime import datetime, timedelta\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 4: CUSTOMER SEGMENTATION USING RFM CLUSTERING\")\nprint(\"=\"*80)\n\n# =================================================================================\n# STEP 1: CALCULATE RFM METRICS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 1: Calculating RFM Metrics\")\nprint(f\"{'-'*80}\")\n\ncurrent_date = df['Order Date'].max() + timedelta(days=1)\n\n# Calculate RFM\nrfm = df.groupby('Customer ID').agg({\n    'Order Date': lambda x: (current_date - x.max()).days,\n    'Order ID': 'count',\n    'Sales': 'sum',\n    'Customer Name': 'first',\n    'Segment': 'first'\n}).reset_index()\n\nrfm.columns = ['Customer_ID', 'Recency', 'Frequency', 'Monetary_Total', \n               'Customer_Name', 'Customer_Segment']\nrfm['Monetary'] = rfm['Monetary_Total'] / rfm['Frequency']\n\nprint(f\"✓ RFM calculated for {len(rfm)} customers\")\nprint(f\"  Recency: {rfm['Recency'].mean():.0f} days (avg)\")\nprint(f\"  Frequency: {rfm['Frequency'].mean():.1f} orders (avg)\")\nprint(f\"  Monetary: ${rfm['Monetary'].mean():,.2f} per order (avg)\")\n\n# =================================================================================\n# STEP 2: NORMALIZATION & OPTIMAL K DETERMINATION\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2: Data Normalization & Optimal K Determination\")\nprint(f\"{'-'*80}\")\n\n# Normalize\nscaler = MinMaxScaler()\nrfm[['R_Norm', 'F_Norm', 'M_Norm']] = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])\nX = rfm[['R_Norm', 'F_Norm', 'M_Norm']].values\n\n# Find optimal k - test multiple metrics\nk_range = range(2, 11)\ninertias = []\nsilhouette_scores = []\ndb_scores = []\n\nprint(f\"\\nTesting k from {min(k_range)} to {max(k_range)}...\")\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X)\n    \n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n# Display metrics\nprint(f\"\\nCluster Quality Metrics:\")\nprint(f\"{'k':<5} {'Inertia':<12} {'Silhouette':<12} {'Davies-Bouldin':<15}\")\nprint(f\"{'-'*50}\")\nfor i, k in enumerate(k_range):\n    print(f\"{k:<5} {inertias[i]:<12.2f} {silhouette_scores[i]:<12.4f} {db_scores[i]:<15.4f}\")\n\noptimal_k_sil = k_range[np.argmax(silhouette_scores)]\noptimal_k_db = k_range[np.argmin(db_scores)]\noptimal_k = 4  # Based on business context and elbow\n\nprint(f\"\\n✓ Optimal k by Silhouette: {optimal_k_sil}\")\nprint(f\"✓ Optimal k by Davies-Bouldin: {optimal_k_db}\")\nprint(f\"→ Selected k={optimal_k} (business context + metrics)\")\n\n# =================================================================================\n# STEP 3: FINAL CLUSTERING\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 3: K-Means Clustering\")\nprint(f\"{'-'*80}\")\n\n# Final clustering\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nrfm['Cluster'] = kmeans.fit_predict(X)\n\nprint(f\"✓ Clustering completed with k={optimal_k}\")\nprint(f\"  Final Silhouette Score: {silhouette_score(X, rfm['Cluster']):.4f}\")\nprint(f\"  Final Davies-Bouldin Index: {davies_bouldin_score(X, rfm['Cluster']):.4f}\")\n\n# =================================================================================\n# STEP 4: CALCULATE CLV SCORES\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 4: CLV Score Calculation\")\nprint(f\"{'-'*80}\")\n\n# AHP Weights\nW_R, W_F, W_M = 0.15, 0.55, 0.30\nrfm['R_Norm_Inv'] = 1 - rfm['R_Norm']\nrfm['CLV_Score'] = (rfm['R_Norm_Inv'] * W_R) + (rfm['F_Norm'] * W_F) + (rfm['M_Norm'] * W_M)\n\nprint(f\"✓ Weights: R={W_R}, F={W_F}, M={W_M}\")\n\n# Cluster summary\ncluster_summary = rfm.groupby('Cluster').agg({\n    'Customer_ID': 'count',\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'Monetary': 'mean',\n    'Monetary_Total': 'sum',\n    'CLV_Score': 'mean'\n}).round(2)\n\ncluster_summary.columns = ['Count', 'Avg_Recency', 'Avg_Freq', 'Avg_Monetary', 'Total_Rev', 'CLV']\ncluster_summary['CLV_Rank'] = cluster_summary['CLV'].rank(ascending=False).astype(int)\ncluster_summary = cluster_summary.sort_values('CLV_Rank')\n\nprint(f\"\\nCluster Summary:\")\nprint(cluster_summary)\n\n# =================================================================================\n# STEP 5: ASSIGN SEGMENTS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 5: Strategic Segmentation\")\nprint(f\"{'-'*80}\")\n\ndef assign_segment(cluster_id, summary):\n    \"\"\"Assign segment based on cluster characteristics\"\"\"\n    row = summary[summary.index == cluster_id].iloc[0]\n    rank = row['CLV_Rank']\n    recency = row['Avg_Recency']\n    freq = row['Avg_Freq']\n    \n    if rank == 1:\n        return 'Champions', 'VIP Retention', '5-10%', 'Quarterly'\n    elif rank == 2 and freq >= summary['Avg_Freq'].median():\n        return 'Loyal Customers', 'Retention & Growth', '10-15%', 'Monthly'\n    elif recency <= summary['Avg_Recency'].median() and rank <= 3:\n        return 'Potential Loyalists', 'Conversion', '15-20%', 'Bi-weekly'\n    elif recency > summary['Avg_Recency'].quantile(0.75):\n        return 'At Risk', 'Win-Back', '20-30%', 'Immediate'\n    else:\n        return 'Need Attention', 'Re-activation', '15-25%', 'Weekly'\n\n# Apply segmentation\nsegment_info = []\nfor cluster in cluster_summary.index:\n    seg_name, strategy, discount, freq = assign_segment(cluster, cluster_summary)\n    segment_info.append({\n        'Cluster': cluster,\n        'Segment': seg_name,\n        'Strategy': strategy,\n        'Discount': discount,\n        'Frequency': freq\n    })\n\nsegment_df = pd.DataFrame(segment_info)\ncluster_summary = cluster_summary.reset_index().merge(segment_df, on='Cluster')\n\n# Map back to customers\nsegment_map = segment_df.set_index('Cluster')['Segment'].to_dict()\nrfm['Segment'] = rfm['Cluster'].map(segment_map)\n\nprint(f\"\\nSegment Distribution:\")\nfor _, row in cluster_summary.iterrows():\n    pct = (row['Count'] / len(rfm)) * 100\n    print(f\"  {row['Segment']:20s} | {row['Count']:4d} customers ({pct:4.1f}%) | \"\n          f\"CLV Rank: {row['CLV_Rank']} | Strategy: {row['Strategy']}\")\n\n# =================================================================================\n# STEP 6: KEY RECOMMENDATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 6: Strategic Recommendations\")\nprint(f\"{'-'*80}\")\n\nrecommendations = {\n    'Champions': [\n        'VIP program with exclusive perks',\n        'Early product access',\n        'Personalized service'\n    ],\n    'Loyal Customers': [\n        'Loyalty points program',\n        'Bundle offers',\n        'Regular engagement'\n    ],\n    'Potential Loyalists': [\n        'Next-purchase incentives',\n        'Product education',\n        'Referral program'\n    ],\n    'At Risk': [\n        'Win-back emails',\n        'Special comeback offers',\n        'Address pain points'\n    ],\n    'Need Attention': [\n        'Promotional campaigns',\n        'Product recommendations',\n        'Re-engagement sequences'\n    ]\n}\n\nrisk_mitigation = {\n    'Champions': 'Monitor churn signals, maintain service quality',\n    'Loyal Customers': 'Track purchase trends, quick issue resolution',\n    'Potential Loyalists': 'Monitor 90-day retention, ensure positive experiences',\n    'At Risk': 'Root cause analysis, competitive monitoring',\n    'Need Attention': 'Evaluate ROI, test messaging approaches'\n}\n\nprint(f\"\\n{'='*80}\")\nfor _, row in cluster_summary.iterrows():\n    seg = row['Segment']\n    print(f\"\\n{seg.upper()} (Cluster {row['Cluster']})\")\n    print(f\"{'-'*80}\")\n    print(f\"  Size: {row['Count']} customers | CLV Rank: {row['CLV_Rank']}\")\n    print(f\"  Profile: R={row['Avg_Recency']:.0f}d, F={row['Avg_Freq']:.1f}, M=${row['Avg_Monetary']:.0f}\")\n    print(f\"  Strategy: {row['Strategy']} | Discount: {row['Discount']} | Frequency: {row['Frequency']}\")\n    print(f\"\\n  Key Actions:\")\n    for action in recommendations.get(seg, ['Standard engagement']):\n        print(f\"    • {action}\")\n    print(f\"\\n  Risk Mitigation: {risk_mitigation.get(seg, 'Monitor performance')}\")\n\n# =================================================================================\n# STEP 7: VISUALIZATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 7: Creating Visualizations\")\nprint(f\"{'-'*80}\")\n\nfig = plt.figure(figsize=(22, 14))\ngs = GridSpec(3, 4, figure=fig, hspace=0.35, wspace=0.35)\n\ncolors = {'Champions': '#2ecc71', 'Loyal Customers': '#3498db', \n          'Potential Loyalists': '#f39c12', 'At Risk': '#e74c3c', \n          'Need Attention': '#9b59b6'}\n\n# Chart 1: Elbow Method\nax1 = fig.add_subplot(gs[0, 0])\nax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\nax1.axvline(x=optimal_k, color='r', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax1.set_xlabel('Number of Clusters (k)', fontsize=10)\nax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=10)\nax1.set_title('Elbow Method for Optimal k', fontsize=12, fontweight='bold')\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 1: Elbow Method\")\n\n# Chart 2: Silhouette Score\nax2 = fig.add_subplot(gs[0, 1])\nax2.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\nax2.axvline(x=optimal_k, color='r', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax2.set_xlabel('Number of Clusters (k)', fontsize=10)\nax2.set_ylabel('Silhouette Score', fontsize=10)\nax2.set_title('Silhouette Score by k\\n(Higher is Better)', fontsize=12, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 2: Silhouette Score\")\n\n# Chart 3: Davies-Bouldin Index\nax3 = fig.add_subplot(gs[0, 2])\nax3.plot(k_range, db_scores, 'mo-', linewidth=2, markersize=8)\nax3.axvline(x=optimal_k, color='r', linestyle='--', linewidth=2, label=f'Selected k={optimal_k}')\nax3.set_xlabel('Number of Clusters (k)', fontsize=10)\nax3.set_ylabel('Davies-Bouldin Index', fontsize=10)\nax3.set_title('Davies-Bouldin Index by k\\n(Lower is Better)', fontsize=12, fontweight='bold')\nax3.legend(fontsize=9)\nax3.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 3: Davies-Bouldin Index\")\n\n# Chart 4: Segment Distribution\nax4 = fig.add_subplot(gs[0, 3])\nseg_counts = rfm['Segment'].value_counts()\ncolors_list = [colors.get(s, '#95a5a6') for s in seg_counts.index]\nwedges, texts, autotexts = ax4.pie(seg_counts.values, labels=seg_counts.index, autopct='%1.1f%%',\n                                     colors=colors_list, startangle=90)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(9)\n    autotext.set_weight('bold')\nax4.set_title('Customer Segment Distribution', fontsize=12, fontweight='bold')\nprint(\"  ✓ Chart 4: Segment Distribution\")\n\n# Chart 5: RFM 3D Scatter\nax5 = fig.add_subplot(gs[1, :2], projection='3d')\nfor seg in rfm['Segment'].unique():\n    seg_data = rfm[rfm['Segment'] == seg]\n    ax5.scatter(seg_data['Recency'], seg_data['Frequency'], seg_data['Monetary'],\n               c=colors.get(seg, '#95a5a6'), label=seg, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\nax5.set_xlabel('Recency (days)', fontsize=9)\nax5.set_ylabel('Frequency (orders)', fontsize=9)\nax5.set_zlabel('Monetary ($)', fontsize=9)\nax5.set_title('RFM 3D Cluster Visualization', fontsize=12, fontweight='bold', pad=20)\nax5.legend(loc='upper right', fontsize=8)\nprint(\"  ✓ Chart 5: 3D Scatter Plot\")\n\n# Chart 6: CLV by Segment\nax6 = fig.add_subplot(gs[1, 2:])\nclv_data = cluster_summary.sort_values('CLV', ascending=True)\ncolors_list = [colors.get(s, '#95a5a6') for s in clv_data['Segment']]\nbars = ax6.barh(range(len(clv_data)), clv_data['CLV'], color=colors_list, alpha=0.7, edgecolor='black')\nax6.set_yticks(range(len(clv_data)))\nax6.set_yticklabels(clv_data['Segment'], fontsize=10)\nfor i, v in enumerate(clv_data['CLV']):\n    ax6.text(v, i, f' {v:.3f}', va='center', fontsize=9, fontweight='bold')\nax6.set_title('CLV Score by Segment', fontsize=12, fontweight='bold')\nax6.set_xlabel('Weighted RFM Score', fontsize=10)\nax6.grid(axis='x', alpha=0.3)\nprint(\"  ✓ Chart 6: CLV Score\")\n\n# Chart 7: Recency vs Frequency\nax7 = fig.add_subplot(gs[2, 0])\nfor seg in rfm['Segment'].unique():\n    seg_data = rfm[rfm['Segment'] == seg]\n    ax7.scatter(seg_data['Recency'], seg_data['Frequency'], \n               c=colors.get(seg, '#95a5a6'), label=seg, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\nax7.set_xlabel('Recency (days)', fontsize=10)\nax7.set_ylabel('Frequency (orders)', fontsize=10)\nax7.set_title('Recency vs Frequency', fontsize=12, fontweight='bold')\nax7.legend(fontsize=7, loc='best')\nax7.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 7: Recency vs Frequency\")\n\n# Chart 8: Frequency vs Monetary\nax8 = fig.add_subplot(gs[2, 1])\nfor seg in rfm['Segment'].unique():\n    seg_data = rfm[rfm['Segment'] == seg]\n    ax8.scatter(seg_data['Frequency'], seg_data['Monetary'],\n               c=colors.get(seg, '#95a5a6'), label=seg, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\nax8.set_xlabel('Frequency (orders)', fontsize=10)\nax8.set_ylabel('Monetary ($)', fontsize=10)\nax8.set_title('Frequency vs Monetary', fontsize=12, fontweight='bold')\nax8.legend(fontsize=7, loc='best')\nax8.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 8: Frequency vs Monetary\")\n\n# Chart 9: Revenue by Segment\nax9 = fig.add_subplot(gs[2, 2])\nrev_data = cluster_summary.sort_values('Total_Rev', ascending=True)\ncolors_list = [colors.get(s, '#95a5a6') for s in rev_data['Segment']]\nbars = ax9.barh(range(len(rev_data)), rev_data['Total_Rev']/1000, color=colors_list, alpha=0.7, edgecolor='black')\nax9.set_yticks(range(len(rev_data)))\nax9.set_yticklabels(rev_data['Segment'], fontsize=10)\nfor i, v in enumerate(rev_data['Total_Rev']/1000):\n    ax9.text(v, i, f' ${v:.0f}K', va='center', fontsize=9, fontweight='bold')\nax9.set_title('Total Revenue by Segment', fontsize=12, fontweight='bold')\nax9.set_xlabel('Revenue ($K)', fontsize=10)\nax9.grid(axis='x', alpha=0.3)\nprint(\"  ✓ Chart 9: Revenue by Segment\")\n\n# Chart 10: RFM Heatmap\nax10 = fig.add_subplot(gs[2, 3])\nheatmap_data = cluster_summary[['Segment', 'Avg_Recency', 'Avg_Freq', 'Avg_Monetary']].set_index('Segment')\n# Normalize for heatmap\nheatmap_norm = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\nheatmap_norm.columns = ['Recency', 'Frequency', 'Monetary']\n# Invert recency for visualization (lower recency = better)\nheatmap_norm['Recency'] = 1 - heatmap_norm['Recency']\nsns.heatmap(heatmap_norm, annot=True, fmt='.2f', cmap='RdYlGn', cbar_kws={'label': 'Normalized Score'},\n           linewidths=1, linecolor='white', ax=ax10, vmin=0, vmax=1)\nax10.set_title('RFM Profile Heatmap', fontsize=12, fontweight='bold')\nax10.set_ylabel('')\nprint(\"  ✓ Chart 10: RFM Heatmap\")\n\nfig.suptitle('RFM Customer Segmentation Analysis\\nK-Means Clustering with Optimal k Determination & CLV Scoring', \n            fontsize=15, fontweight='bold', y=0.99)\n\nplt.tight_layout()\nplt.savefig('rfm_clustering_analysis.png', dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Visualization saved: rfm_clustering_analysis.png\")\nplt.show()\n\n# =================================================================================\n# SUMMARY METRICS\n# =================================================================================\nprint(f\"\\n{'='*80}\")\nprint(\"SUMMARY METRICS\")\nprint(f\"{'='*80}\")\n\ntotal_customers = len(rfm)\ntotal_revenue = rfm['Monetary_Total'].sum()\n\nprint(f\"\\nOverall Performance:\")\nprint(f\"  Total Customers: {total_customers:,}\")\nprint(f\"  Total Revenue: ${total_revenue:,.2f}\")\nprint(f\"  Avg CLV Score: {rfm['CLV_Score'].mean():.4f}\")\n\nprint(f\"\\nTop Performers:\")\nchampions = rfm[rfm['Segment'] == 'Champions']\nif len(champions) > 0:\n    champ_rev = champions['Monetary_Total'].sum()\n    champ_pct = (champ_rev / total_revenue) * 100\n    print(f\"  Champions: {len(champions)} customers ({len(champions)/total_customers*100:.1f}%)\")\n    print(f\"  Champions Revenue: ${champ_rev:,.2f} ({champ_pct:.1f}% of total)\")\n\nat_risk = rfm[rfm['Segment'] == 'At Risk']\nif len(at_risk) > 0:\n    risk_rev = at_risk['Monetary_Total'].sum()\n    print(f\"\\nAt-Risk Customers:\")\n    print(f\"  Count: {len(at_risk)} customers\")\n    print(f\"  Revenue at Risk: ${risk_rev:,.2f}\")\n    print(f\"  → Immediate win-back campaign recommended\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ SECTION 4: RFM CUSTOMER SEGMENTATION COMPLETE\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:16:27.322704Z","iopub.execute_input":"2026-02-09T08:16:27.32347Z","iopub.status.idle":"2026-02-09T08:16:35.06164Z","shell.execute_reply.started":"2026-02-09T08:16:27.323432Z","shell.execute_reply":"2026-02-09T08:16:35.060718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5) GEOGRAPHIC & MARKET ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"Section 5 focuses on regional, state, and city-level performance to guide targeted marketing and advertising strategies. Key metrics include total sales, unique customers, sales per customer, and segment/category performance.","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# SECTION 5: GEOGRAPHIC & MARKET ANALYSIS\n# Regional Performance, Market Penetration, and Advertising Target Recommendations\n# =================================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 5: GEOGRAPHIC & MARKET ANALYSIS\")\nprint(\"Regional Performance & Advertising Target Recommendations\")\nprint(\"=\"*80)\n\n# =================================================================================\n# STEP 1: REGIONAL PERFORMANCE ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 1: Regional Performance Analysis\")\nprint(f\"{'-'*80}\")\n\n# Aggregate by Region\nregion_perf = df.groupby('Region').agg({\n    'Sales': ['sum', 'mean', 'count'],\n    'Order ID': 'nunique',\n    'Customer ID': 'nunique',\n    'Product ID': 'nunique'\n}).reset_index()\n\nregion_perf.columns = ['Region', 'Total_Sales', 'Avg_Order_Value', 'Total_Transactions',\n                        'Unique_Orders', 'Unique_Customers', 'Unique_Products']\n\n# Calculate metrics\ntotal_sales = region_perf['Total_Sales'].sum()\nregion_perf['Market_Share_%'] = (region_perf['Total_Sales'] / total_sales) * 100\nregion_perf['Sales_Per_Customer'] = region_perf['Total_Sales'] / region_perf['Unique_Customers']\nregion_perf = region_perf.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nRegional Performance Summary:\")\nprint(region_perf[['Region', 'Total_Sales', 'Market_Share_%', 'Unique_Customers', \n                   'Sales_Per_Customer']].to_string(index=False))\n\nprint(f\"\\nKey Regional Insights:\")\nfor idx, row in region_perf.iterrows():\n    print(f\"  {row['Region']}:\")\n    print(f\"    - Market Share: {row['Market_Share_%']:.2f}%\")\n    print(f\"    - Customers: {int(row['Unique_Customers'])}\")\n    print(f\"    - Avg Sales/Customer: ${row['Sales_Per_Customer']:,.2f}\")\n\n# =================================================================================\n# STEP 2: STATE-LEVEL PERFORMANCE ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2: State-Level Performance Analysis\")\nprint(f\"{'-'*80}\")\n\nstate_perf = df.groupby(['Region', 'State']).agg({\n    'Sales': ['sum', 'mean', 'count'],\n    'Customer ID': 'nunique',\n    'Product ID': 'nunique'\n}).reset_index()\n\nstate_perf.columns = ['Region', 'State', 'Total_Sales', 'Avg_Transaction', \n                       'Total_Transactions', 'Unique_Customers', 'Unique_Products']\n\nstate_perf['Sales_Per_Customer'] = state_perf['Total_Sales'] / state_perf['Unique_Customers']\nstate_perf = state_perf.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nTop 15 States by Sales:\")\nprint(state_perf.head(15)[['State', 'Region', 'Total_Sales', 'Unique_Customers', \n                            'Sales_Per_Customer']].to_string(index=False))\n\nprint(\"\\nBottom 10 States by Sales:\")\nprint(state_perf.tail(10)[['State', 'Region', 'Total_Sales', 'Unique_Customers',\n                            'Sales_Per_Customer']].to_string(index=False))\n\n# =================================================================================\n# STEP 3: CITY-LEVEL PERFORMANCE ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 3: City-Level Performance Analysis\")\nprint(f\"{'-'*80}\")\n\ncity_perf = df.groupby(['Region', 'State', 'City']).agg({\n    'Sales': ['sum', 'mean', 'count'],\n    'Customer ID': 'nunique'\n}).reset_index()\n\ncity_perf.columns = ['Region', 'State', 'City', 'Total_Sales', 'Avg_Transaction',\n                      'Total_Transactions', 'Unique_Customers']\n\ncity_perf['Sales_Per_Customer'] = city_perf['Total_Sales'] / city_perf['Unique_Customers']\ncity_perf = city_perf.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nTop 20 Cities by Sales:\")\nprint(city_perf.head(20)[['City', 'State', 'Region', 'Total_Sales', \n                           'Unique_Customers', 'Sales_Per_Customer']].to_string(index=False))\n\n# =================================================================================\n# STEP 4: SEGMENT PERFORMANCE BY REGION\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 4: Customer Segment Performance by Region\")\nprint(f\"{'-'*80}\")\n\nregion_segment_perf = df.groupby(['Region', 'Segment']).agg({\n    'Sales': 'sum',\n    'Customer ID': 'nunique',\n    'Order ID': 'count'\n}).reset_index()\n\nregion_segment_perf.columns = ['Region', 'Segment', 'Total_Sales', \n                                 'Unique_Customers', 'Total_Orders']\n\nregion_segment_pivot = region_segment_perf.pivot(index='Region', \n                                                   columns='Segment', \n                                                   values='Total_Sales').fillna(0)\n\nprint(\"\\nSales by Region and Customer Segment:\")\nprint(region_segment_pivot.to_string())\n\nprint(\"\\nDominant Customer Segment by Region:\")\nfor region in region_segment_pivot.index:\n    dominant_segment = region_segment_pivot.loc[region].idxmax()\n    dominant_value = region_segment_pivot.loc[region].max()\n    total_regional_sales = region_segment_pivot.loc[region].sum()\n    dominance_pct = (dominant_value / total_regional_sales) * 100\n    print(f\"  {region}: {dominant_segment} (${dominant_value:,.2f} - {dominance_pct:.1f}%)\")\n\n# =================================================================================\n# STEP 5: CATEGORY PERFORMANCE BY REGION\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 5: Product Category Performance by Region\")\nprint(f\"{'-'*80}\")\n\nregion_category_perf = df.groupby(['Region', 'Category']).agg({\n    'Sales': 'sum',\n    'Order ID': 'count'\n}).reset_index()\n\nregion_category_perf.columns = ['Region', 'Category', 'Total_Sales', 'Total_Orders']\n\nregion_category_pivot = region_category_perf.pivot(index='Region', \n                                                     columns='Category', \n                                                     values='Total_Sales').fillna(0)\n\nprint(\"\\nCategory Sales by Region:\")\nprint(region_category_pivot.to_string())\n\nprint(\"\\nTop 5 Sub-Categories by Region:\")\nfor region in df['Region'].unique():\n    region_data = df[df['Region'] == region]\n    top_subcat = region_data.groupby('Sub-Category')['Sales'].sum().nlargest(5)\n    print(f\"\\n  {region}:\")\n    for subcat, sales in top_subcat.items():\n        print(f\"    - {subcat}: ${sales:,.2f}\")\n\n# =================================================================================\n# STEP 6: SUB-CATEGORY BY SEGMENT\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 6: Sub-Category Performance by Customer Segment\")\nprint(f\"{'-'*80}\")\n\nprint(\"\\nTop 5 Sub-Categories by Customer Segment:\")\nfor segment in df['Segment'].unique():\n    segment_data = df[df['Segment'] == segment]\n    top_subcat = segment_data.groupby('Sub-Category')['Sales'].sum().nlargest(5)\n    print(f\"\\n  {segment}:\")\n    for subcat, sales in top_subcat.items():\n        print(f\"    - {subcat}: ${sales:,.2f}\")\n\n# =================================================================================\n# STEP 7: MARKET OPPORTUNITY CLASSIFICATION\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 7: Market Opportunity Analysis\")\nprint(f\"{'-'*80}\")\n\nmedian_sales = state_perf['Total_Sales'].median()\nmedian_customers = state_perf['Unique_Customers'].median()\n\ndef classify_market_type(row):\n    if row['Total_Sales'] >= median_sales and row['Unique_Customers'] >= median_customers:\n        return 'Established Market'\n    elif row['Total_Sales'] >= median_sales and row['Unique_Customers'] < median_customers:\n        return 'Growth Market'\n    elif row['Total_Sales'] < median_sales and row['Unique_Customers'] >= median_customers:\n        return 'Emerging Market'\n    else:\n        return 'Untapped Market'\n\nstate_perf['Market_Type'] = state_perf.apply(classify_market_type, axis=1)\n\nmarket_summary = state_perf.groupby('Market_Type').agg({\n    'State': 'count',\n    'Total_Sales': 'sum',\n    'Unique_Customers': 'sum',\n    'Sales_Per_Customer': 'mean'\n}).round(2)\n\nmarket_summary.columns = ['State_Count', 'Total_Sales', 'Total_Customers', 'Avg_Sales_Per_Customer']\n\nprint(\"\\nMarket Type Summary:\")\nprint(market_summary)\n\nprint(\"\\nStates by Market Type:\")\nfor market_type in ['Established Market', 'Growth Market', 'Emerging Market', 'Untapped Market']:\n    states = state_perf[state_perf['Market_Type'] == market_type]['State'].tolist()\n    print(f\"\\n  {market_type} ({len(states)} states):\")\n    print(f\"    {', '.join(states[:10])}\")\n    if len(states) > 10:\n        print(f\"    ... and {len(states)-10} more\")\n\n# =================================================================================\n# STEP 8: ADVERTISING STRATEGY\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 8: Advertising Target Recommendations\")\nprint(f\"{'-'*80}\")\n\ndef get_ad_strategy(row):\n    market_type = row['Market_Type']\n    sales_per_customer = row['Sales_Per_Customer']\n    \n    if market_type == 'Established Market':\n        return {\n            'priority': 'High',\n            'strategy': 'Retention & Upsell',\n            'budget_%': 30,\n            'channels': 'Email, Loyalty Programs, Retargeting',\n            'message': 'Premium products, VIP benefits',\n            'discount': '5-10%'\n        }\n    elif market_type == 'Growth Market':\n        return {\n            'priority': 'Very High',\n            'strategy': 'Aggressive Acquisition',\n            'budget_%': 35,\n            'channels': 'Social Media, Google Ads, Influencer',\n            'message': 'New customer discounts, referrals',\n            'discount': '15-20%'\n        }\n    elif market_type == 'Emerging Market':\n        return {\n            'priority': 'High',\n            'strategy': 'Basket Size Increase',\n            'budget_%': 25,\n            'channels': 'Email, Cross-sell Campaigns',\n            'message': 'Bundle deals, upsell',\n            'discount': '10-15%'\n        }\n    else:\n        return {\n            'priority': 'Low',\n            'strategy': 'Market Testing',\n            'budget_%': 10,\n            'channels': 'Low-cost Digital',\n            'message': 'Brand awareness',\n            'discount': '20-25%'\n        }\n\nad_data = state_perf.apply(get_ad_strategy, axis=1)\n\nstate_perf['Ad_Priority'] = ad_data.apply(lambda x: x['priority'])\nstate_perf['Ad_Strategy'] = ad_data.apply(lambda x: x['strategy'])\nstate_perf['Budget_%'] = ad_data.apply(lambda x: x['budget_%'])\nstate_perf['Channels'] = ad_data.apply(lambda x: x['channels'])\nstate_perf['Message'] = ad_data.apply(lambda x: x['message'])\nstate_perf['Discount'] = ad_data.apply(lambda x: x['discount'])\n\nprint(\"\\nAdvertising Strategy by State (Top 20):\")\nprint(state_perf.nlargest(20, 'Total_Sales')[\n    ['State', 'Region', 'Market_Type', 'Ad_Priority', 'Ad_Strategy', 'Budget_%', 'Discount']\n].to_string(index=False))\n\n# =================================================================================\n# STEP 9: REGIONAL CAMPAIGN PLAYBOOK\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 9: Regional Campaign Recommendations\")\nprint(f\"{'-'*80}\")\n\nprint(\"\\nREGIONAL TARGETING PLAYBOOK:\")\nprint(f\"{'='*80}\")\n\nfor region in df['Region'].unique():\n    print(f\"\\n{region.upper()} REGION:\")\n    region_data = df[df['Region'] == region]\n    region_info = region_perf[region_perf['Region'] == region].iloc[0]\n    \n    dominant_segment = region_data.groupby('Segment')['Sales'].sum().idxmax()\n    top_categories = region_data.groupby('Category')['Sales'].sum().nlargest(2)\n    top_subcats = region_data.groupby('Sub-Category')['Sales'].sum().nlargest(3)\n    top_cities = region_data.groupby('City')['Sales'].sum().nlargest(3)\n    \n    print(f\"  Market Share: {region_info['Market_Share_%']:.2f}%\")\n    print(f\"  Dominant Segment: {dominant_segment}\")\n    print(f\"  Top Categories: {', '.join(top_categories.index.tolist())}\")\n    print(f\"  Top Sub-Categories: {', '.join(top_subcats.index.tolist())}\")\n    print(f\"  Top Cities: {', '.join(top_cities.index.tolist())}\")\n    print(f\"\\n  CAMPAIGN RECOMMENDATION:\")\n    print(f\"    Target: {dominant_segment} customers\")\n    print(f\"    Products: {top_subcats.index[0]}, {top_subcats.index[1]}\")\n    print(f\"    Cities: {top_cities.index[0]}, {top_cities.index[1]}\")\n\n# =================================================================================\n# STEP 10: VISUALIZATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 10: Creating Visualizations\")\nprint(f\"{'-'*80}\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\nfig = plt.figure(figsize=(24, 20))\ngs = GridSpec(4, 3, figure=fig, hspace=0.35, wspace=0.3)\n\nregion_colors = {'West': '#3498db', 'East': '#e74c3c', \n                 'Central': '#2ecc71', 'South': '#f39c12'}\n\n# Chart 1: Regional Market Share\nax1 = fig.add_subplot(gs[0, 0])\ncolors = [region_colors.get(r, '#95a5a6') for r in region_perf['Region']]\nwedges, texts, autotexts = ax1.pie(region_perf['Total_Sales'], labels=region_perf['Region'],\n                                     autopct='%1.1f%%', colors=colors, startangle=90,\n                                     explode=[0.05]*len(region_perf))\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(10)\n    autotext.set_weight('bold')\nax1.set_title('Regional Market Share', fontsize=12, fontweight='bold')\nprint(\"  ✓ Chart 1: Regional Market Share\")\n\n# Chart 2: Regional Sales vs Customers\nax2 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(region_perf))\nwidth = 0.35\ncolors_list = [region_colors.get(r, '#95a5a6') for r in region_perf['Region']]\nbars1 = ax2.bar(x - width/2, region_perf['Total_Sales']/1000, width, \n                label='Sales ($1000s)', color=colors_list, alpha=0.7, edgecolor='black')\nax2_twin = ax2.twinx()\nbars2 = ax2_twin.bar(x + width/2, region_perf['Unique_Customers'], width,\n                     label='Customers', color='lightcoral', alpha=0.6, edgecolor='black')\nax2.set_xticks(x)\nax2.set_xticklabels(region_perf['Region'], fontsize=9)\nax2.set_title('Regional Sales vs Customers', fontsize=12, fontweight='bold')\nax2.set_ylabel('Sales ($1000s)', fontsize=10)\nax2_twin.set_ylabel('Customers', fontsize=10)\nax2.legend(loc='upper left', fontsize=9)\nax2_twin.legend(loc='upper right', fontsize=9)\nax2.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 2: Sales vs Customers\")\n\n# Chart 3: Sales per Customer by Region\nax3 = fig.add_subplot(gs[0, 2])\nbars = ax3.bar(region_perf['Region'], region_perf['Sales_Per_Customer'],\n               color=colors_list, alpha=0.7, edgecolor='black')\nfor bar in bars:\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n            f'${height:,.0f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\nax3.set_title('Sales per Customer by Region', fontsize=12, fontweight='bold')\nax3.set_ylabel('Sales per Customer ($)', fontsize=10)\nax3.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 3: Sales per Customer\")\n\n# Chart 4: Top 15 States\nax4 = fig.add_subplot(gs[1, :])\ntop_states = state_perf.nlargest(15, 'Total_Sales').sort_values('Total_Sales')\nstate_colors_map = dict(zip(region_perf['Region'], colors_list))\nbar_colors = [state_colors_map.get(r, '#95a5a6') for r in top_states['Region']]\nbars = ax4.barh(range(len(top_states)), top_states['Total_Sales'],\n                color=bar_colors, alpha=0.7, edgecolor='black')\nax4.set_yticks(range(len(top_states)))\nax4.set_yticklabels([f\"{row['State']} ({row['Region']})\" \n                      for _, row in top_states.iterrows()], fontsize=9)\nfor i, (idx, row) in enumerate(top_states.iterrows()):\n    ax4.text(row['Total_Sales'], i, f\" ${row['Total_Sales']:,.0f}\", \n            va='center', fontsize=8, fontweight='bold')\nax4.set_title('Top 15 States by Sales', fontsize=12, fontweight='bold')\nax4.set_xlabel('Total Sales ($)', fontsize=10)\nax4.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax4.grid(axis='x', alpha=0.3)\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=region_colors[reg], label=reg, alpha=0.7) \n                   for reg in region_colors.keys()]\nax4.legend(handles=legend_elements, loc='lower right', fontsize=9, title='Region')\nprint(\"  ✓ Chart 4: Top States\")\n\n# Chart 5: Top 20 Cities\nax5 = fig.add_subplot(gs[2, :])\ntop_cities = city_perf.nlargest(20, 'Total_Sales').sort_values('Total_Sales')\nbar_colors = [state_colors_map.get(r, '#95a5a6') for r in top_cities['Region']]\nbars = ax5.barh(range(len(top_cities)), top_cities['Total_Sales'],\n                color=bar_colors, alpha=0.7, edgecolor='black')\nax5.set_yticks(range(len(top_cities)))\nax5.set_yticklabels([f\"{row['City']}, {row['State']}\" \n                      for _, row in top_cities.iterrows()], fontsize=8)\nfor i, (idx, row) in enumerate(top_cities.iterrows()):\n    ax5.text(row['Total_Sales'], i, f\" ${row['Total_Sales']:,.0f}\", \n            va='center', fontsize=7, fontweight='bold')\nax5.set_title('Top 20 Cities by Sales', fontsize=12, fontweight='bold')\nax5.set_xlabel('Total Sales ($)', fontsize=10)\nax5.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax5.grid(axis='x', alpha=0.3)\nprint(\"  ✓ Chart 5: Top Cities\")\n\n# Chart 6: Segment by Region\nax6 = fig.add_subplot(gs[3, 0])\nregion_segment_pivot.plot(kind='bar', stacked=True, ax=ax6, \n                          color=['#3498db', '#e74c3c', '#2ecc71'],\n                          alpha=0.7, edgecolor='black', linewidth=0.5)\nax6.set_title('Segment Sales by Region', fontsize=12, fontweight='bold')\nax6.set_xlabel('Region', fontsize=10)\nax6.set_ylabel('Sales ($)', fontsize=10)\nax6.legend(title='Segment', fontsize=8, loc='upper left')\nax6.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax6.set_xticklabels(ax6.get_xticklabels(), rotation=0)\nax6.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 6: Segment by Region\")\n\n# Chart 7: Category by Region\nax7 = fig.add_subplot(gs[3, 1])\nregion_category_pivot.plot(kind='bar', stacked=True, ax=ax7,\n                            color=['#3498db', '#e74c3c', '#2ecc71'],\n                            alpha=0.7, edgecolor='black', linewidth=0.5)\nax7.set_title('Category Sales by Region', fontsize=12, fontweight='bold')\nax7.set_xlabel('Region', fontsize=10)\nax7.set_ylabel('Sales ($)', fontsize=10)\nax7.legend(title='Category', fontsize=8, loc='upper left')\nax7.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax7.set_xticklabels(ax7.get_xticklabels(), rotation=0)\nax7.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 7: Category by Region\")\n\n# Chart 8: Market Type Distribution\nax8 = fig.add_subplot(gs[3, 2])\nmarket_counts = state_perf['Market_Type'].value_counts()\nmarket_colors = ['#2ecc71', '#3498db', '#f39c12', '#95a5a6']\nwedges, texts, autotexts = ax8.pie(market_counts.values, labels=market_counts.index,\n                                     autopct='%1.1f%%', colors=market_colors, startangle=90)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(8)\n    autotext.set_weight('bold')\nax8.set_title('Market Type Distribution', fontsize=12, fontweight='bold')\nprint(\"  ✓ Chart 8: Market Types\")\n\nfig.suptitle('Geographic & Market Analysis Dashboard\\nRegional Performance & Advertising Strategy', \n            fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('geographic_market_analysis.png', dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Saved: geographic_market_analysis.png\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:17:04.384073Z","iopub.execute_input":"2026-02-09T08:17:04.385201Z","iopub.status.idle":"2026-02-09T08:17:10.084397Z","shell.execute_reply.started":"2026-02-09T08:17:04.38513Z","shell.execute_reply":"2026-02-09T08:17:10.08313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6) CUSTOMER CLUSTERING & PACKAGE BUNDLING ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"Section 6 is focused on customer segmentation and tailored promotional strategies.\nIt uses behavioral clustering to identify distinct customer groups and develops package bundling recommendations to increase engagement and revenue.","metadata":{}},{"cell_type":"code","source":"# =================================================================================\n# SECTION 6: CUSTOMER CLUSTERING & PACKAGE BUNDLING ANALYSIS\n# Segment × Shipping Mode × Order Frequency × Transaction Value Analysis\n# =================================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 6: CUSTOMER CLUSTERING & PACKAGE BUNDLING ANALYSIS\")\nprint(\"Promotional Package Recommendations by Customer Behavior\")\nprint(\"=\"*80)\n\n# =================================================================================\n# STEP 1: CUSTOMER BEHAVIORAL DATA PREPARATION\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 1: Customer Behavioral Data Preparation\")\nprint(f\"{'-'*80}\")\n\n# Aggregate customer behavior metrics\ncustomer_behavior = df.groupby('Customer ID').agg({\n    'Order ID': 'count',                    # Order Frequency\n    'Sales': ['sum', 'mean'],               # Transaction Value\n    'Ship Mode': lambda x: x.mode()[0],     # Preferred Shipping Mode\n    'Segment': 'first',                     # Customer Segment\n    'Customer Name': 'first',\n    'Category': lambda x: x.mode()[0],      # Most Purchased Category\n    'Sub-Category': lambda x: x.mode()[0],  # Most Purchased Sub-Category\n    'Order Date': lambda x: (x.max() - x.min()).days  # Customer Tenure\n}).reset_index()\n\ncustomer_behavior.columns = ['Customer_ID', 'Order_Frequency', 'Total_Sales', \n                              'Avg_Transaction_Value', 'Preferred_Ship_Mode',\n                              'Segment', 'Customer_Name', 'Top_Category', \n                              'Top_SubCategory', 'Tenure_Days']\n\nprint(f\"\\n✓ Customer behavioral data prepared for {len(customer_behavior)} customers\")\nprint(f\"\\nBehavioral Metrics Summary:\")\nprint(f\"  Order Frequency: {customer_behavior['Order_Frequency'].mean():.1f} avg orders\")\nprint(f\"  Avg Transaction Value: ${customer_behavior['Avg_Transaction_Value'].mean():,.2f}\")\nprint(f\"  Customer Tenure: {customer_behavior['Tenure_Days'].mean():.0f} days avg\")\n\n# =================================================================================\n# STEP 2: SHIPPING MODE ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 2: Shipping Mode Analysis\")\nprint(f\"{'-'*80}\")\n\nshipping_analysis = df.groupby(['Ship Mode', 'Segment']).agg({\n    'Sales': ['sum', 'mean', 'count'],\n    'Customer ID': 'nunique'\n}).reset_index()\n\nshipping_analysis.columns = ['Ship_Mode', 'Segment', 'Total_Sales', \n                              'Avg_Order_Value', 'Total_Orders', 'Unique_Customers']\n\nshipping_analysis['Orders_Per_Customer'] = (shipping_analysis['Total_Orders'] / \n                                             shipping_analysis['Unique_Customers'])\n\nprint(\"\\nShipping Mode Performance by Segment:\")\nshipping_pivot = shipping_analysis.pivot_table(\n    index='Ship_Mode', \n    columns='Segment', \n    values='Total_Sales', \n    fill_value=0\n)\nprint(shipping_pivot.to_string())\n\n# Overall shipping mode preference\nship_mode_summary = df.groupby('Ship Mode').agg({\n    'Sales': ['sum', 'count'],\n    'Customer ID': 'nunique'\n}).reset_index()\n\nship_mode_summary.columns = ['Ship_Mode', 'Total_Sales', 'Total_Orders', 'Unique_Customers']\nship_mode_summary['Market_Share_%'] = (ship_mode_summary['Total_Sales'] / \n                                        ship_mode_summary['Total_Sales'].sum()) * 100\n\nprint(\"\\nShipping Mode Market Share:\")\nprint(ship_mode_summary[['Ship_Mode', 'Total_Sales', 'Total_Orders', 'Market_Share_%']].to_string(index=False))\n\n# =================================================================================\n# STEP 3: SEGMENT × SHIPPING MODE × CATEGORY ANALYSIS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 3: Segment × Shipping Mode × Category Cross-Analysis\")\nprint(f\"{'-'*80}\")\n\n# Three-way analysis\ncross_analysis = df.groupby(['Segment', 'Ship Mode', 'Category']).agg({\n    'Sales': ['sum', 'count', 'mean'],\n    'Customer ID': 'nunique'\n}).reset_index()\n\ncross_analysis.columns = ['Segment', 'Ship_Mode', 'Category', 'Total_Sales', \n                          'Order_Count', 'Avg_Order_Value', 'Unique_Customers']\n\ncross_analysis = cross_analysis.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nTop 20 Segment-Shipping-Category Combinations:\")\nprint(cross_analysis.head(20)[['Segment', 'Ship_Mode', 'Category', 'Total_Sales', \n                                'Order_Count', 'Avg_Order_Value']].to_string(index=False))\n\n# =================================================================================\n# STEP 4: CUSTOMER CLUSTERING BASED ON BEHAVIOR\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 4: K-Means Clustering on Customer Behavior\")\nprint(f\"{'-'*80}\")\n\n# Prepare features for clustering\nclustering_features = customer_behavior[['Order_Frequency', 'Total_Sales', \n                                          'Avg_Transaction_Value', 'Tenure_Days']].copy()\n\n# Handle missing values\nclustering_features = clustering_features.fillna(0)\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(clustering_features)\n\n# Determine optimal number of clusters using elbow method\ninertias = []\nK_range = range(3, 9)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(features_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Use 5 clusters for detailed segmentation\noptimal_k = 5\nprint(f\"\\n✓ Using {optimal_k} clusters for customer segmentation\")\n\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ncustomer_behavior['Cluster'] = kmeans_final.fit_predict(features_scaled)\n\n# Analyze cluster characteristics\ncluster_profile = customer_behavior.groupby('Cluster').agg({\n    'Customer_ID': 'count',\n    'Order_Frequency': 'mean',\n    'Total_Sales': 'mean',\n    'Avg_Transaction_Value': 'mean',\n    'Tenure_Days': 'mean'\n}).round(2)\n\ncluster_profile.columns = ['Customer_Count', 'Avg_Order_Freq', 'Avg_Total_Sales',\n                            'Avg_Transaction_Value', 'Avg_Tenure_Days']\n\nprint(\"\\nCluster Profiles:\")\nprint(cluster_profile)\n\n# Name clusters based on characteristics\ndef name_cluster(row):\n    \"\"\"Assign meaningful names to clusters based on behavior\"\"\"\n    cluster_id = row.name\n    order_freq = cluster_profile.loc[cluster_id, 'Avg_Order_Freq']\n    total_sales = cluster_profile.loc[cluster_id, 'Avg_Total_Sales']\n    avg_trans = cluster_profile.loc[cluster_id, 'Avg_Transaction_Value']\n    \n    # High frequency, high value\n    if order_freq > cluster_profile['Avg_Order_Freq'].quantile(0.75) and \\\n       total_sales > cluster_profile['Avg_Total_Sales'].quantile(0.75):\n        return 'Premium High-Frequency'\n    \n    # High value, low frequency\n    elif total_sales > cluster_profile['Avg_Total_Sales'].quantile(0.75) and \\\n         order_freq < cluster_profile['Avg_Order_Freq'].median():\n        return 'High-Value Occasional'\n    \n    # High frequency, low value\n    elif order_freq > cluster_profile['Avg_Order_Freq'].quantile(0.75) and \\\n         avg_trans < cluster_profile['Avg_Transaction_Value'].median():\n        return 'Frequent Small-Basket'\n    \n    # Medium all-around\n    elif order_freq > cluster_profile['Avg_Order_Freq'].quantile(0.25) and \\\n         total_sales > cluster_profile['Avg_Total_Sales'].quantile(0.25):\n        return 'Standard Regular'\n    \n    # Low engagement\n    else:\n        return 'Low-Engagement'\n\ncluster_profile['Cluster_Name'] = cluster_profile.apply(name_cluster, axis=1)\n\n# Map cluster names back to customer behavior\ncluster_name_map = cluster_profile['Cluster_Name'].to_dict()\ncustomer_behavior['Cluster_Name'] = customer_behavior['Cluster'].map(cluster_name_map)\n\nprint(\"\\nNamed Cluster Profiles:\")\nprint(cluster_profile)\n\n# =================================================================================\n# STEP 5: PACKAGE BUNDLING STRATEGY DEVELOPMENT\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 5: Package Bundling Strategy Development\")\nprint(f\"{'-'*80}\")\n\n# Analyze popular product combinations by cluster\nprint(\"\\nTop Product Combinations by Cluster:\")\n\nfor cluster_id in range(optimal_k):\n    cluster_name = cluster_name_map[cluster_id]\n    print(f\"\\n{cluster_name.upper()} (Cluster {cluster_id}):\")\n    \n    # Get customers in this cluster\n    cluster_customers = customer_behavior[customer_behavior['Cluster'] == cluster_id]['Customer_ID'].tolist()\n    \n    # Get their orders\n    cluster_orders = df[df['Customer ID'].isin(cluster_customers)]\n    \n    # Top categories\n    top_categories = cluster_orders.groupby('Category')['Sales'].sum().nlargest(3)\n    print(f\"  Top Categories: {', '.join(top_categories.index.tolist())}\")\n    \n    # Top sub-categories\n    top_subcats = cluster_orders.groupby('Sub-Category')['Sales'].sum().nlargest(5)\n    print(f\"  Top Sub-Categories:\")\n    for subcat, sales in top_subcats.items():\n        print(f\"    - {subcat}: ${sales:,.2f}\")\n    \n    # Preferred shipping mode\n    top_ship = cluster_orders.groupby('Ship Mode')['Order ID'].count().idxmax()\n    ship_pct = (cluster_orders[cluster_orders['Ship Mode'] == top_ship].shape[0] / \n                cluster_orders.shape[0]) * 100\n    print(f\"  Preferred Shipping: {top_ship} ({ship_pct:.1f}%)\")\n    \n    # Dominant segment\n    top_segment = cluster_orders.groupby('Segment')['Customer ID'].nunique().idxmax()\n    print(f\"  Dominant Segment: {top_segment}\")\n\n# =================================================================================\n# STEP 6: PROMOTIONAL PACKAGE RECOMMENDATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 6: Promotional Package Recommendations\")\nprint(f\"{'-'*80}\")\n\ndef create_package_recommendation(cluster_id, customer_behavior_df, df_original):\n    \"\"\"Create detailed package recommendations for each cluster\"\"\"\n    \n    cluster_name = cluster_name_map[cluster_id]\n    cluster_customers = customer_behavior_df[customer_behavior_df['Cluster'] == cluster_id]['Customer_ID'].tolist()\n    cluster_orders = df_original[df_original['Customer ID'].isin(cluster_customers)]\n    \n    # Cluster metrics\n    cluster_metrics = cluster_profile.loc[cluster_id]\n    \n    # Top products\n    top_subcats = cluster_orders.groupby('Sub-Category')['Sales'].sum().nlargest(5)\n    top_categories = cluster_orders.groupby('Category')['Sales'].sum().nlargest(3)\n    \n    # Shipping preference\n    ship_preference = cluster_orders.groupby('Ship Mode')['Order ID'].count()\n    preferred_ship = ship_preference.idxmax()\n    \n    # Segment distribution\n    segment_dist = cluster_orders.groupby('Segment')['Customer ID'].nunique()\n    dominant_segment = segment_dist.idxmax()\n    \n    # Calculate package value\n    avg_basket = cluster_metrics['Avg_Transaction_Value']\n    \n    # Determine discount and package structure\n    if cluster_name == 'Premium High-Frequency':\n        return {\n            'cluster_name': cluster_name,\n            'customer_count': cluster_metrics['Customer_Count'],\n            'package_name': 'VIP Exclusive Bundle',\n            'target_segment': dominant_segment,\n            'shipping_mode': preferred_ship,\n            'shipping_discount': 'Free Shipping',\n            'product_bundle': f\"{top_subcats.index[0]} + {top_subcats.index[1]} + {top_subcats.index[2]}\",\n            'bundle_discount_%': 10,\n            'package_price': avg_basket * 2.5 * 0.90,  # 2.5x items with 10% discount\n            'promo_frequency': 'Monthly',\n            'value_proposition': 'Premium bundle for loyal customers with free express shipping',\n            'expected_uplift_%': 25\n        }\n    \n    elif cluster_name == 'High-Value Occasional':\n        return {\n            'cluster_name': cluster_name,\n            'customer_count': cluster_metrics['Customer_Count'],\n            'package_name': 'Premium Combo Deal',\n            'target_segment': dominant_segment,\n            'shipping_mode': preferred_ship,\n            'shipping_discount': 'Free Standard Shipping on $200+',\n            'product_bundle': f\"{top_subcats.index[0]} + {top_subcats.index[1]}\",\n            'bundle_discount_%': 15,\n            'package_price': avg_basket * 1.8 * 0.85,  # 1.8x items with 15% discount\n            'promo_frequency': 'Quarterly',\n            'value_proposition': 'High-value bundles to increase purchase frequency',\n            'expected_uplift_%': 30\n        }\n    \n    elif cluster_name == 'Frequent Small-Basket':\n        return {\n            'cluster_name': cluster_name,\n            'customer_count': cluster_metrics['Customer_Count'],\n            'package_name': 'Value Pack Special',\n            'target_segment': dominant_segment,\n            'shipping_mode': preferred_ship,\n            'shipping_discount': '$5 Flat Rate Shipping',\n            'product_bundle': f\"{top_subcats.index[0]} (x3) + {top_subcats.index[1]} (x2)\",\n            'bundle_discount_%': 20,\n            'package_price': avg_basket * 3 * 0.80,  # 3x items with 20% discount\n            'promo_frequency': 'Weekly',\n            'value_proposition': 'Bulk savings to increase basket size',\n            'expected_uplift_%': 40\n        }\n    \n    elif cluster_name == 'Standard Regular':\n        return {\n            'cluster_name': cluster_name,\n            'customer_count': cluster_metrics['Customer_Count'],\n            'package_name': 'Essential Bundle',\n            'target_segment': dominant_segment,\n            'shipping_mode': preferred_ship,\n            'shipping_discount': 'Free Shipping on $100+',\n            'product_bundle': f\"{top_subcats.index[0]} + {top_subcats.index[1]} + Free Gift\",\n            'bundle_discount_%': 12,\n            'package_price': avg_basket * 2 * 0.88,  # 2x items with 12% discount\n            'promo_frequency': 'Bi-weekly',\n            'value_proposition': 'Standard value bundles with free gift incentive',\n            'expected_uplift_%': 20\n        }\n    \n    else:  # Low-Engagement\n        return {\n            'cluster_name': cluster_name,\n            'customer_count': cluster_metrics['Customer_Count'],\n            'package_name': 'Starter Pack',\n            'target_segment': dominant_segment,\n            'shipping_mode': 'Standard Class',  # Cost-effective\n            'shipping_discount': '$10 off shipping on first bundle',\n            'product_bundle': f\"{top_subcats.index[0]} + Trial Products\",\n            'bundle_discount_%': 25,\n            'package_price': avg_basket * 1.5 * 0.75,  # 1.5x items with 25% discount\n            'promo_frequency': 'One-time Activation',\n            'value_proposition': 'Aggressive discount to re-engage dormant customers',\n            'expected_uplift_%': 50\n        }\n\n# Generate package recommendations\npackage_recommendations = []\n\nfor cluster_id in range(optimal_k):\n    recommendation = create_package_recommendation(cluster_id, customer_behavior, df)\n    package_recommendations.append(recommendation)\n\n# Convert to DataFrame\npackages_df = pd.DataFrame(package_recommendations)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PROMOTIONAL PACKAGE RECOMMENDATIONS\")\nprint(\"=\"*80)\n\nfor idx, package in packages_df.iterrows():\n    print(f\"\\n{'─'*80}\")\n    print(f\"PACKAGE {idx+1}: {package['package_name'].upper()}\")\n    print(f\"{'─'*80}\")\n    print(f\"Target Cluster: {package['cluster_name']}\")\n    print(f\"Customer Base: {int(package['customer_count'])} customers\")\n    print(f\"Target Segment: {package['target_segment']}\")\n    print(f\"\\nBUNDLE DETAILS:\")\n    print(f\"  Products: {package['product_bundle']}\")\n    print(f\"  Discount: {package['bundle_discount_%']}% off\")\n    print(f\"  Package Price: ${package['package_price']:,.2f}\")\n    print(f\"\\nSHIPPING:\")\n    print(f\"  Preferred Mode: {package['shipping_mode']}\")\n    print(f\"  Shipping Offer: {package['shipping_discount']}\")\n    print(f\"\\nPROMOTION STRATEGY:\")\n    print(f\"  Frequency: {package['promo_frequency']}\")\n    print(f\"  Value Proposition: {package['value_proposition']}\")\n    print(f\"  Expected Revenue Uplift: {package['expected_uplift_%']}%\")\n\n# =================================================================================\n# STEP 7: DETAILED SEGMENT-SHIPPING PACKAGE MATRIX\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 7: Segment-Shipping Mode Package Matrix\")\nprint(f\"{'-'*80}\")\n\n# Create detailed combinations\nsegment_ship_packages = []\n\nfor segment in df['Segment'].unique():\n    for ship_mode in df['Ship Mode'].unique():\n        \n        # Filter data\n        subset = df[(df['Segment'] == segment) & (df['Ship Mode'] == ship_mode)]\n        \n        if len(subset) == 0:\n            continue\n        \n        # Get top products\n        top_products = subset.groupby('Sub-Category')['Sales'].sum().nlargest(3)\n        \n        # Calculate metrics\n        total_sales = subset['Sales'].sum()\n        order_count = subset['Order ID'].nunique()\n        avg_order = total_sales / order_count if order_count > 0 else 0\n        customer_count = subset['Customer ID'].nunique()\n        \n        # Determine package strategy\n        if avg_order > subset['Sales'].mean():\n            package_type = 'Premium Bundle'\n            discount = 10\n        elif order_count > df['Order ID'].nunique() * 0.1:\n            package_type = 'Popular Pack'\n            discount = 15\n        else:\n            package_type = 'Starter Set'\n            discount = 20\n        \n        segment_ship_packages.append({\n            'Segment': segment,\n            'Ship_Mode': ship_mode,\n            'Package_Type': package_type,\n            'Top_Product_1': top_products.index[0] if len(top_products) > 0 else 'N/A',\n            'Top_Product_2': top_products.index[1] if len(top_products) > 1 else 'N/A',\n            'Top_Product_3': top_products.index[2] if len(top_products) > 2 else 'N/A',\n            'Customer_Count': customer_count,\n            'Total_Sales': total_sales,\n            'Avg_Order_Value': avg_order,\n            'Discount_%': discount,\n            'Bundle_Price': avg_order * 2 * (1 - discount/100)\n        })\n\nsegment_ship_df = pd.DataFrame(segment_ship_packages)\nsegment_ship_df = segment_ship_df.sort_values('Total_Sales', ascending=False)\n\nprint(\"\\nTop 15 Segment-Shipping Combinations with Package Recommendations:\")\nprint(segment_ship_df.head(15)[['Segment', 'Ship_Mode', 'Package_Type', \n                                 'Top_Product_1', 'Top_Product_2', \n                                 'Discount_%', 'Bundle_Price']].to_string(index=False))\n\n# =================================================================================\n# STEP 8: VISUALIZATIONS\n# =================================================================================\nprint(f\"\\n{'-'*80}\")\nprint(\"STEP 8: Creating Clustering & Package Visualizations\")\nprint(f\"{'-'*80}\")\n\nplt.style.use('seaborn-v0_8-whitegrid')\nfig = plt.figure(figsize=(24, 18))\ngs = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.3)\n\ncluster_colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n\n# Chart 1: Cluster Distribution\nax1 = fig.add_subplot(gs[0, 0])\ncluster_counts = customer_behavior['Cluster_Name'].value_counts()\nwedges, texts, autotexts = ax1.pie(cluster_counts.values, labels=cluster_counts.index,\n                                     autopct='%1.1f%%', colors=cluster_colors,\n                                     startangle=90)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(9)\n    autotext.set_weight('bold')\nfor text in texts:\n    text.set_fontsize(8)\nax1.set_title('Customer Cluster Distribution', fontsize=12, fontweight='bold')\nprint(\"  ✓ Chart 1: Cluster Distribution\")\n\n# Chart 2: Cluster Profiles (Radar Chart alternative - Bar Chart)\nax2 = fig.add_subplot(gs[0, 1:])\ncluster_profile_plot = cluster_profile[['Avg_Order_Freq', 'Avg_Total_Sales', \n                                         'Avg_Transaction_Value']].copy()\ncluster_profile_plot['Avg_Total_Sales'] = cluster_profile_plot['Avg_Total_Sales'] / 1000  # Scale\n\nx = np.arange(len(cluster_profile_plot))\nwidth = 0.25\n\nbars1 = ax2.bar(x - width, cluster_profile_plot['Avg_Order_Freq'], \n                width, label='Avg Order Freq', color='#3498db', alpha=0.7)\nbars2 = ax2.bar(x, cluster_profile_plot['Avg_Total_Sales'], \n                width, label='Avg Total Sales ($1000s)', color='#e74c3c', alpha=0.7)\nbars3 = ax2.bar(x + width, cluster_profile_plot['Avg_Transaction_Value']/100, \n                width, label='Avg Trans Value ($100s)', color='#2ecc71', alpha=0.7)\n\nax2.set_xticks(x)\nax2.set_xticklabels([cluster_name_map[i] for i in range(optimal_k)], \n                     rotation=15, ha='right', fontsize=9)\nax2.set_title('Cluster Behavioral Profiles', fontsize=12, fontweight='bold')\nax2.set_ylabel('Value', fontsize=10)\nax2.legend(fontsize=9)\nax2.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 2: Cluster Profiles\")\n\n# Chart 3: Shipping Mode Distribution\nax3 = fig.add_subplot(gs[1, 0])\nship_colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\nwedges, texts, autotexts = ax3.pie(ship_mode_summary['Total_Sales'], \n                                     labels=ship_mode_summary['Ship_Mode'],\n                                     autopct='%1.1f%%', colors=ship_colors,\n                                     startangle=90)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(9)\n    autotext.set_weight('bold')\nax3.set_title('Shipping Mode Market Share', fontsize=12, fontweight='bold')\nprint(\"  ✓ Chart 3: Shipping Mode Distribution\")\n\n# Chart 4: Segment × Shipping Mode Heatmap\nax4 = fig.add_subplot(gs[1, 1:])\nheatmap_data = shipping_analysis.pivot_table(index='Ship_Mode', columns='Segment', \n                                               values='Total_Sales', fill_value=0)\nsns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='YlOrRd', \n            cbar_kws={'label': 'Total Sales ($)'}, linewidths=1, \n            linecolor='white', ax=ax4)\nax4.set_title('Segment × Shipping Mode Sales Heatmap', fontsize=12, fontweight='bold')\nax4.set_xlabel('Customer Segment', fontsize=10)\nax4.set_ylabel('Shipping Mode', fontsize=10)\nprint(\"  ✓ Chart 4: Segment-Shipping Heatmap\")\n\n# Chart 5: PCA Visualization of Clusters\nax5 = fig.add_subplot(gs[2, 0])\npca = PCA(n_components=2)\nfeatures_pca = pca.fit_transform(features_scaled)\n\nfor cluster_id in range(optimal_k):\n    cluster_data = features_pca[customer_behavior['Cluster'] == cluster_id]\n    ax5.scatter(cluster_data[:, 0], cluster_data[:, 1], \n               c=cluster_colors[cluster_id], label=cluster_name_map[cluster_id],\n               alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n\nax5.set_title('Customer Clusters (PCA Visualization)', fontsize=12, fontweight='bold')\nax5.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=10)\nax5.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=10)\nax5.legend(fontsize=8, loc='best')\nax5.grid(True, alpha=0.3)\nprint(\"  ✓ Chart 5: PCA Cluster Visualization\")\n\n# Chart 6: Package Discount Strategy\nax6 = fig.add_subplot(gs[2, 1])\nbars = ax6.barh(packages_df['cluster_name'], packages_df['bundle_discount_%'],\n                color=cluster_colors, alpha=0.7, edgecolor='black')\nfor i, (idx, row) in enumerate(packages_df.iterrows()):\n    ax6.text(row['bundle_discount_%'], i, f\" {row['bundle_discount_%']}%\",\n            va='center', fontsize=9, fontweight='bold')\nax6.set_title('Bundle Discount by Cluster', fontsize=12, fontweight='bold')\nax6.set_xlabel('Discount %', fontsize=10)\nax6.grid(axis='x', alpha=0.3)\nprint(\"  ✓ Chart 6: Package Discounts\")\n\n# Chart 7: Expected Revenue Uplift\nax7 = fig.add_subplot(gs[2, 2])\nbars = ax7.bar(range(len(packages_df)), packages_df['expected_uplift_%'],\n               color=cluster_colors, alpha=0.7, edgecolor='black')\nax7.set_xticks(range(len(packages_df)))\nax7.set_xticklabels([name[:15] for name in packages_df['cluster_name']], \n                     rotation=45, ha='right', fontsize=8)\nfor i, val in enumerate(packages_df['expected_uplift_%']):\n    ax7.text(i, val, f'{val}%', ha='center', va='bottom', \n            fontsize=9, fontweight='bold')\nax7.set_title('Expected Revenue Uplift by Package', fontsize=12, fontweight='bold')\nax7.set_ylabel('Uplift %', fontsize=10)\nax7.grid(axis='y', alpha=0.3)\nprint(\"  ✓ Chart 7: Revenue Uplift\")\n\nfig.suptitle('Customer Clustering & Package Bundling Analysis\\nPromotional Package Recommendations', \n            fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('customer_clustering_package_analysis.png', dpi=300, bbox_inches='tight')\nprint(f\"\\n✓ Saved: customer_clustering_package_analysis.png\")\nplt.show()\n\n\nprint(f\"\\n{'='*80}\")\nprint(\"✓ SECTION 6: CUSTOMER CLUSTERING & PACKAGE BUNDLING COMPLETE\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:17:37.058376Z","iopub.execute_input":"2026-02-09T08:17:37.058918Z","iopub.status.idle":"2026-02-09T08:17:42.076751Z","shell.execute_reply.started":"2026-02-09T08:17:37.058879Z","shell.execute_reply":"2026-02-09T08:17:42.075715Z"}},"outputs":[],"execution_count":null}]}
